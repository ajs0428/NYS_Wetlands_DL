{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5c3bf5-9201-4c7f-b5b6-7622df105d1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/Anthony/Data and Analysis Local/NYS_Wetlands_DL\n",
      "\n",
      "Data Configuration:\n",
      "  data_dir: Data/Patches_v2\n",
      "  cluster_id: 208\n",
      "  huc_id: All HUCs in cluster\n",
      "\n",
      "Training Configuration:\n",
      "  num_epochs: 25\n",
      "  batch_size: 10\n",
      "  learning_rate: 0.001\n",
      "  base_filters: 32\n",
      "  output_dir: Models\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "workdir = Path(\"/Users/Anthony/Data and Analysis Local/NYS_Wetlands_DL/\")\n",
    "os.chdir(workdir)\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Must match the output from NYS_03_create_patches_v2.ipynb\n",
    "data_dir = Path(\"Data/Patches_v2\")\n",
    "cluster_id = 208  # Cluster to load, or None for legacy files\n",
    "huc_id = None     # Specific HUC to load, or None to combine all HUCs in cluster\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 25\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "base_filters = 32  # U-Net base filter count\n",
    "\n",
    "# Output directory for models\n",
    "output_dir = Path(\"Models\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\nData Configuration:\")\n",
    "print(f\"  data_dir: {data_dir}\")\n",
    "print(f\"  cluster_id: {cluster_id}\")\n",
    "print(f\"  huc_id: {huc_id or 'All HUCs in cluster'}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  num_epochs: {num_epochs}\")\n",
    "print(f\"  batch_size: {batch_size}\")\n",
    "print(f\"  learning_rate: {learning_rate}\")\n",
    "print(f\"  base_filters: {base_filters}\")\n",
    "print(f\"  output_dir: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b38b106-673a-4eb4-8628-95db182d4e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Found 9 training file(s)\n",
      "Found 9 validation file(s)\n",
      "\n",
      "Dataset Summary:\n",
      "  Training batches: 264\n",
      "  Validation batches: 67\n",
      "  in_channels: 11\n",
      "  num_classes: 2\n",
      "  band_names: ['r', 'g', 'b', 'nir', 'ndvi', 'ndwi', 'dem', 'chm', 'slope_5m', 'TPI_5m', 'Geomorph_5m']\n",
      "  HUCs included: ['metadata', 'metadata', 'metadata', 'metadata', 'metadata', 'metadata', 'metadata', 'metadata', 'metadata']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Add script directory to Python path\n",
    "script_dir = Path(\"Python_Code_Analysis/DL_Implement/\")\n",
    "sys.path.insert(0, str(script_dir))\n",
    "\n",
    "# Import modules\n",
    "from NYS_04_dataset import get_dataloaders, find_patch_files, load_and_merge_metadata\n",
    "from NYS_05_unet_model import UNet\n",
    "\n",
    "# === LOAD DATA ===\n",
    "print(\"Loading data...\")\n",
    "train_loader, val_loader, metadata = get_dataloaders(\n",
    "    data_dir, \n",
    "    cluster_id=cluster_id, \n",
    "    huc_id=huc_id, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  in_channels: {metadata['in_channels']}\")\n",
    "print(f\"  num_classes: {metadata['num_classes']}\")\n",
    "print(f\"  band_names: {metadata['band_names']}\")\n",
    "if \"hucs_included\" in metadata:\n",
    "    print(f\"  HUCs included: {metadata['hucs_included']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c47603f5-d35e-44d1-89fc-07c44c8787df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove"
    ]
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch and return average loss.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Progress update every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"    Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, metadata):\n",
    "    \"\"\"Validate and return loss plus per-class accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    num_classes = metadata[\"num_classes\"]\n",
    "    class_names = metadata[\"class_names\"]\n",
    "\n",
    "    # Track correct predictions per class\n",
    "    correct_per_class = torch.zeros(num_classes)\n",
    "    total_per_class = torch.zeros(num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Per-class accuracy\n",
    "            for c in range(num_classes):\n",
    "                mask = (y == c)\n",
    "                total_per_class[c] += mask.sum().item()\n",
    "                correct_per_class[c] += ((preds == c) & mask).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "\n",
    "    # Calculate per-class accuracy\n",
    "    class_acc = {}\n",
    "    for c in range(num_classes):\n",
    "        if total_per_class[c] > 0:\n",
    "            class_acc[class_names[c]] = correct_per_class[c] / total_per_class[c]\n",
    "        else:\n",
    "            class_acc[class_names[c]] = 0.0\n",
    "\n",
    "    # Overall accuracy\n",
    "    overall_acc = correct_per_class.sum() / total_per_class.sum()\n",
    "\n",
    "    return avg_loss, overall_acc.item(), class_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8691be53-c8a1-4658-a540-61b84a1765ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading y_train files for class weight computation...\n",
      "  Combined y_train shape: (2638, 128, 128)\n",
      "\n",
      "Class distribution and weights:\n",
      "  Background: 33,693,033 pixels (77.96%) -> weight: 1.00\n",
      "  WET: 9,527,959 pixels (22.04%) -> weight: 3.54\n",
      "\n",
      "Initializing model...\n",
      "Total parameters: 7,768,322\n",
      "\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "Epoch 1/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.5698\n",
      "    Batch 20/264, Loss: 0.6446\n",
      "    Batch 30/264, Loss: 0.5459\n",
      "    Batch 40/264, Loss: 0.5621\n",
      "    Batch 50/264, Loss: 0.4295\n",
      "    Batch 60/264, Loss: 0.5064\n",
      "    Batch 70/264, Loss: 0.5398\n",
      "    Batch 80/264, Loss: 0.4876\n",
      "    Batch 90/264, Loss: 0.4512\n",
      "    Batch 100/264, Loss: 0.5994\n",
      "    Batch 110/264, Loss: 0.5440\n",
      "    Batch 120/264, Loss: 0.4364\n",
      "    Batch 130/264, Loss: 0.4473\n",
      "    Batch 140/264, Loss: 0.4740\n",
      "    Batch 150/264, Loss: 0.5143\n",
      "    Batch 160/264, Loss: 0.5341\n",
      "    Batch 170/264, Loss: 0.6187\n",
      "    Batch 180/264, Loss: 0.4147\n",
      "    Batch 190/264, Loss: 0.5753\n",
      "    Batch 200/264, Loss: 0.7274\n",
      "    Batch 210/264, Loss: 0.4664\n",
      "    Batch 220/264, Loss: 0.5203\n",
      "    Batch 230/264, Loss: 0.4592\n",
      "    Batch 240/264, Loss: 0.4719\n",
      "    Batch 250/264, Loss: 0.4766\n",
      "    Batch 260/264, Loss: 0.4234\n",
      "\n",
      "  Train Loss: 0.5095\n",
      "  Val Loss:   0.4631\n",
      "  Val Acc:    0.7821\n",
      "  Time:       27.0s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7741\n",
      "    WET: 0.8094\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 2/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.5882\n",
      "    Batch 20/264, Loss: 0.6433\n",
      "    Batch 30/264, Loss: 0.3620\n",
      "    Batch 40/264, Loss: 0.4145\n",
      "    Batch 50/264, Loss: 0.3601\n",
      "    Batch 60/264, Loss: 0.4771\n",
      "    Batch 70/264, Loss: 0.4404\n",
      "    Batch 80/264, Loss: 0.3838\n",
      "    Batch 90/264, Loss: 0.3852\n",
      "    Batch 100/264, Loss: 0.6202\n",
      "    Batch 110/264, Loss: 0.4723\n",
      "    Batch 120/264, Loss: 0.4356\n",
      "    Batch 130/264, Loss: 0.5073\n",
      "    Batch 140/264, Loss: 0.4760\n",
      "    Batch 150/264, Loss: 0.4428\n",
      "    Batch 160/264, Loss: 0.3648\n",
      "    Batch 170/264, Loss: 0.5200\n",
      "    Batch 180/264, Loss: 0.5257\n",
      "    Batch 190/264, Loss: 0.5640\n",
      "    Batch 200/264, Loss: 0.4944\n",
      "    Batch 210/264, Loss: 0.4262\n",
      "    Batch 220/264, Loss: 0.7343\n",
      "    Batch 230/264, Loss: 0.4181\n",
      "    Batch 240/264, Loss: 0.4078\n",
      "    Batch 250/264, Loss: 0.3943\n",
      "    Batch 260/264, Loss: 0.4944\n",
      "\n",
      "  Train Loss: 0.4779\n",
      "  Val Loss:   0.4600\n",
      "  Val Acc:    0.7706\n",
      "  Time:       17.5s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7532\n",
      "    WET: 0.8299\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 3/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.3983\n",
      "    Batch 20/264, Loss: 0.3619\n",
      "    Batch 30/264, Loss: 0.4163\n",
      "    Batch 40/264, Loss: 0.6703\n",
      "    Batch 50/264, Loss: 0.4239\n",
      "    Batch 60/264, Loss: 0.4368\n",
      "    Batch 70/264, Loss: 0.4258\n",
      "    Batch 80/264, Loss: 0.4452\n",
      "    Batch 90/264, Loss: 0.5044\n",
      "    Batch 100/264, Loss: 0.4349\n",
      "    Batch 110/264, Loss: 0.5017\n",
      "    Batch 120/264, Loss: 0.3656\n",
      "    Batch 130/264, Loss: 0.5029\n",
      "    Batch 140/264, Loss: 0.4049\n",
      "    Batch 150/264, Loss: 0.9803\n",
      "    Batch 160/264, Loss: 0.3660\n",
      "    Batch 170/264, Loss: 0.4527\n",
      "    Batch 180/264, Loss: 0.4287\n",
      "    Batch 190/264, Loss: 0.3788\n",
      "    Batch 200/264, Loss: 0.3678\n",
      "    Batch 210/264, Loss: 0.3929\n",
      "    Batch 220/264, Loss: 0.4156\n",
      "    Batch 230/264, Loss: 0.4766\n",
      "    Batch 240/264, Loss: 0.6962\n",
      "    Batch 250/264, Loss: 0.5254\n",
      "    Batch 260/264, Loss: 0.5320\n",
      "\n",
      "  Train Loss: 0.4702\n",
      "  Val Loss:   0.4524\n",
      "  Val Acc:    0.7972\n",
      "  Time:       17.5s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7964\n",
      "    WET: 0.7998\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 4/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.4408\n",
      "    Batch 20/264, Loss: 0.4338\n",
      "    Batch 30/264, Loss: 0.3431\n",
      "    Batch 40/264, Loss: 0.3320\n",
      "    Batch 50/264, Loss: 0.4045\n",
      "    Batch 60/264, Loss: 0.3481\n",
      "    Batch 70/264, Loss: 0.2949\n",
      "    Batch 80/264, Loss: 0.3677\n",
      "    Batch 90/264, Loss: 0.5249\n",
      "    Batch 100/264, Loss: 0.4941\n",
      "    Batch 110/264, Loss: 0.7512\n",
      "    Batch 120/264, Loss: 0.3478\n",
      "    Batch 130/264, Loss: 0.5300\n",
      "    Batch 140/264, Loss: 0.5725\n",
      "    Batch 150/264, Loss: 0.5359\n",
      "    Batch 160/264, Loss: 0.4727\n",
      "    Batch 170/264, Loss: 0.3618\n",
      "    Batch 180/264, Loss: 0.4293\n",
      "    Batch 190/264, Loss: 0.4166\n",
      "    Batch 200/264, Loss: 0.4079\n",
      "    Batch 210/264, Loss: 0.3480\n",
      "    Batch 220/264, Loss: 0.3122\n",
      "    Batch 230/264, Loss: 0.4101\n",
      "    Batch 240/264, Loss: 0.5161\n",
      "    Batch 250/264, Loss: 0.4690\n",
      "    Batch 260/264, Loss: 0.3797\n",
      "\n",
      "  Train Loss: 0.4618\n",
      "  Val Loss:   0.4427\n",
      "  Val Acc:    0.7498\n",
      "  Time:       17.5s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7077\n",
      "    WET: 0.8932\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 5/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.3688\n",
      "    Batch 20/264, Loss: 0.3760\n",
      "    Batch 30/264, Loss: 0.5054\n",
      "    Batch 40/264, Loss: 0.4942\n",
      "    Batch 50/264, Loss: 0.3745\n",
      "    Batch 60/264, Loss: 0.3833\n",
      "    Batch 70/264, Loss: 0.5020\n",
      "    Batch 80/264, Loss: 0.3791\n",
      "    Batch 90/264, Loss: 0.4597\n",
      "    Batch 100/264, Loss: 0.3007\n",
      "    Batch 110/264, Loss: 0.4054\n",
      "    Batch 120/264, Loss: 0.5606\n",
      "    Batch 130/264, Loss: 0.5287\n",
      "    Batch 140/264, Loss: 0.4680\n",
      "    Batch 150/264, Loss: 0.5262\n",
      "    Batch 160/264, Loss: 0.3932\n",
      "    Batch 170/264, Loss: 0.6206\n",
      "    Batch 180/264, Loss: 0.6522\n",
      "    Batch 190/264, Loss: 0.3567\n",
      "    Batch 200/264, Loss: 0.5537\n",
      "    Batch 210/264, Loss: 0.3393\n",
      "    Batch 220/264, Loss: 0.4142\n",
      "    Batch 230/264, Loss: 0.3919\n",
      "    Batch 240/264, Loss: 0.5219\n",
      "    Batch 250/264, Loss: 0.5089\n",
      "    Batch 260/264, Loss: 0.6043\n",
      "\n",
      "  Train Loss: 0.4674\n",
      "  Val Loss:   0.4454\n",
      "  Val Acc:    0.7860\n",
      "  Time:       17.6s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7728\n",
      "    WET: 0.8313\n",
      "\n",
      "Epoch 6/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.4726\n",
      "    Batch 20/264, Loss: 0.5035\n",
      "    Batch 30/264, Loss: 0.4854\n",
      "    Batch 40/264, Loss: 0.5007\n",
      "    Batch 50/264, Loss: 0.4137\n",
      "    Batch 60/264, Loss: 0.4041\n",
      "    Batch 70/264, Loss: 0.4406\n",
      "    Batch 80/264, Loss: 0.4186\n",
      "    Batch 90/264, Loss: 0.3624\n",
      "    Batch 100/264, Loss: 0.5601\n",
      "    Batch 110/264, Loss: 0.4791\n",
      "    Batch 120/264, Loss: 0.4808\n",
      "    Batch 130/264, Loss: 0.6075\n",
      "    Batch 140/264, Loss: 0.4853\n",
      "    Batch 150/264, Loss: 0.3523\n",
      "    Batch 160/264, Loss: 0.6105\n",
      "    Batch 170/264, Loss: 0.3558\n",
      "    Batch 180/264, Loss: 0.3658\n",
      "    Batch 190/264, Loss: 0.4268\n",
      "    Batch 200/264, Loss: 0.4371\n",
      "    Batch 210/264, Loss: 0.5451\n",
      "    Batch 220/264, Loss: 0.5035\n",
      "    Batch 230/264, Loss: 0.3928\n",
      "    Batch 240/264, Loss: 0.4158\n",
      "    Batch 250/264, Loss: 0.4184\n",
      "    Batch 260/264, Loss: 0.4148\n",
      "\n",
      "  Train Loss: 0.4555\n",
      "  Val Loss:   0.4396\n",
      "  Val Acc:    0.8031\n",
      "  Time:       17.8s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.8114\n",
      "    WET: 0.7745\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 7/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.3649\n",
      "    Batch 20/264, Loss: 0.4556\n",
      "    Batch 30/264, Loss: 0.3301\n",
      "    Batch 40/264, Loss: 0.7485\n",
      "    Batch 50/264, Loss: 0.4593\n",
      "    Batch 60/264, Loss: 0.3444\n",
      "    Batch 70/264, Loss: 0.4525\n",
      "    Batch 80/264, Loss: 0.3949\n",
      "    Batch 90/264, Loss: 0.5673\n",
      "    Batch 100/264, Loss: 0.4080\n",
      "    Batch 110/264, Loss: 0.4580\n",
      "    Batch 120/264, Loss: 0.5960\n",
      "    Batch 130/264, Loss: 0.4208\n",
      "    Batch 140/264, Loss: 0.4799\n",
      "    Batch 150/264, Loss: 0.3967\n",
      "    Batch 160/264, Loss: 0.4772\n",
      "    Batch 170/264, Loss: 0.7961\n",
      "    Batch 180/264, Loss: 0.3665\n",
      "    Batch 190/264, Loss: 0.4948\n",
      "    Batch 200/264, Loss: 0.3666\n",
      "    Batch 210/264, Loss: 0.3138\n",
      "    Batch 220/264, Loss: 0.4094\n",
      "    Batch 230/264, Loss: 0.4126\n",
      "    Batch 240/264, Loss: 0.5764\n",
      "    Batch 250/264, Loss: 0.3851\n",
      "    Batch 260/264, Loss: 0.5472\n",
      "\n",
      "  Train Loss: 0.4543\n",
      "  Val Loss:   0.4459\n",
      "  Val Acc:    0.7482\n",
      "  Time:       17.6s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7064\n",
      "    WET: 0.8906\n",
      "\n",
      "Epoch 8/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.4128\n",
      "    Batch 20/264, Loss: 0.3754\n",
      "    Batch 30/264, Loss: 0.5774\n",
      "    Batch 40/264, Loss: 0.3372\n",
      "    Batch 50/264, Loss: 0.3683\n",
      "    Batch 60/264, Loss: 0.5365\n",
      "    Batch 70/264, Loss: 0.3907\n",
      "    Batch 80/264, Loss: 0.4692\n",
      "    Batch 90/264, Loss: 0.4217\n",
      "    Batch 100/264, Loss: 0.4170\n",
      "    Batch 110/264, Loss: 0.5256\n",
      "    Batch 120/264, Loss: 0.3827\n",
      "    Batch 130/264, Loss: 0.4447\n",
      "    Batch 140/264, Loss: 0.4573\n",
      "    Batch 150/264, Loss: 0.3241\n",
      "    Batch 160/264, Loss: 0.3131\n",
      "    Batch 170/264, Loss: 0.3988\n",
      "    Batch 180/264, Loss: 0.5032\n",
      "    Batch 190/264, Loss: 0.3665\n",
      "    Batch 200/264, Loss: 0.4716\n",
      "    Batch 210/264, Loss: 0.2941\n",
      "    Batch 220/264, Loss: 0.6370\n",
      "    Batch 230/264, Loss: 0.7067\n",
      "    Batch 240/264, Loss: 0.5398\n",
      "    Batch 250/264, Loss: 0.4387\n",
      "    Batch 260/264, Loss: 0.3692\n",
      "\n",
      "  Train Loss: 0.4493\n",
      "  Val Loss:   0.4505\n",
      "  Val Acc:    0.7996\n",
      "  Time:       17.7s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.8009\n",
      "    WET: 0.7954\n",
      "\n",
      "Epoch 9/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.6021\n",
      "    Batch 20/264, Loss: 0.3965\n",
      "    Batch 30/264, Loss: 0.4123\n",
      "    Batch 40/264, Loss: 0.2895\n",
      "    Batch 50/264, Loss: 0.4314\n",
      "    Batch 60/264, Loss: 0.3616\n",
      "    Batch 70/264, Loss: 0.3066\n",
      "    Batch 80/264, Loss: 0.4132\n",
      "    Batch 90/264, Loss: 0.3887\n",
      "    Batch 100/264, Loss: 0.5327\n",
      "    Batch 110/264, Loss: 0.4060\n",
      "    Batch 120/264, Loss: 0.4591\n",
      "    Batch 130/264, Loss: 0.5328\n",
      "    Batch 140/264, Loss: 0.4811\n",
      "    Batch 150/264, Loss: 0.3291\n",
      "    Batch 160/264, Loss: 0.4298\n",
      "    Batch 170/264, Loss: 0.4061\n",
      "    Batch 180/264, Loss: 0.3458\n",
      "    Batch 190/264, Loss: 0.3970\n",
      "    Batch 200/264, Loss: 0.4760\n",
      "    Batch 210/264, Loss: 0.5060\n",
      "    Batch 220/264, Loss: 0.4368\n",
      "    Batch 230/264, Loss: 0.4790\n",
      "    Batch 240/264, Loss: 0.5060\n",
      "    Batch 250/264, Loss: 0.4706\n",
      "    Batch 260/264, Loss: 0.4105\n",
      "\n",
      "  Train Loss: 0.4499\n",
      "  Val Loss:   0.4392\n",
      "  Val Acc:    0.7595\n",
      "  Time:       17.5s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7299\n",
      "    WET: 0.8607\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 10/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.4113\n",
      "    Batch 20/264, Loss: 0.4388\n",
      "    Batch 30/264, Loss: 0.3889\n",
      "    Batch 40/264, Loss: 0.3209\n",
      "    Batch 50/264, Loss: 0.6181\n",
      "    Batch 60/264, Loss: 0.5305\n",
      "    Batch 70/264, Loss: 0.5173\n",
      "    Batch 80/264, Loss: 0.3396\n",
      "    Batch 90/264, Loss: 0.4645\n",
      "    Batch 100/264, Loss: 0.4075\n",
      "    Batch 110/264, Loss: 0.4975\n",
      "    Batch 120/264, Loss: 0.5389\n",
      "    Batch 130/264, Loss: 0.3604\n",
      "    Batch 140/264, Loss: 0.3770\n",
      "    Batch 150/264, Loss: 0.6470\n",
      "    Batch 160/264, Loss: 0.4808\n",
      "    Batch 170/264, Loss: 0.3685\n",
      "    Batch 180/264, Loss: 0.4407\n",
      "    Batch 190/264, Loss: 0.3460\n",
      "    Batch 200/264, Loss: 0.6167\n",
      "    Batch 210/264, Loss: 0.5162\n",
      "    Batch 220/264, Loss: 0.4228\n",
      "    Batch 230/264, Loss: 0.3443\n",
      "    Batch 240/264, Loss: 0.4258\n",
      "    Batch 250/264, Loss: 0.2895\n",
      "    Batch 260/264, Loss: 0.3323\n",
      "\n",
      "  Train Loss: 0.4506\n",
      "  Val Loss:   0.4675\n",
      "  Val Acc:    0.8201\n",
      "  Time:       17.6s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.8535\n",
      "    WET: 0.7063\n",
      "\n",
      "Epoch 11/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.4854\n",
      "    Batch 20/264, Loss: 0.5249\n",
      "    Batch 30/264, Loss: 0.5143\n",
      "    Batch 40/264, Loss: 0.4119\n",
      "    Batch 50/264, Loss: 0.3569\n",
      "    Batch 60/264, Loss: 0.3775\n",
      "    Batch 70/264, Loss: 0.3251\n",
      "    Batch 80/264, Loss: 0.3030\n",
      "    Batch 90/264, Loss: 0.4700\n",
      "    Batch 100/264, Loss: 0.5827\n",
      "    Batch 110/264, Loss: 0.4595\n",
      "    Batch 120/264, Loss: 0.3706\n",
      "    Batch 130/264, Loss: 0.8246\n",
      "    Batch 140/264, Loss: 0.4999\n",
      "    Batch 150/264, Loss: 0.5258\n",
      "    Batch 160/264, Loss: 0.3603\n",
      "    Batch 170/264, Loss: 0.4913\n",
      "    Batch 180/264, Loss: 0.4458\n",
      "    Batch 190/264, Loss: 0.3847\n",
      "    Batch 200/264, Loss: 0.4642\n",
      "    Batch 210/264, Loss: 0.4511\n",
      "    Batch 220/264, Loss: 0.3830\n",
      "    Batch 230/264, Loss: 0.4086\n",
      "    Batch 240/264, Loss: 0.4410\n",
      "    Batch 250/264, Loss: 0.3404\n",
      "    Batch 260/264, Loss: 0.4700\n",
      "\n",
      "  Train Loss: 0.4452\n",
      "  Val Loss:   0.4448\n",
      "  Val Acc:    0.7369\n",
      "  Time:       17.5s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6896\n",
      "    WET: 0.8978\n",
      "\n",
      "Epoch 12/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.3257\n",
      "    Batch 20/264, Loss: 0.4789\n",
      "    Batch 30/264, Loss: 0.3105\n",
      "    Batch 40/264, Loss: 0.3873\n",
      "    Batch 50/264, Loss: 0.5179\n",
      "    Batch 60/264, Loss: 0.3919\n",
      "    Batch 70/264, Loss: 0.3457\n",
      "    Batch 80/264, Loss: 0.4593\n",
      "    Batch 90/264, Loss: 0.5255\n",
      "    Batch 100/264, Loss: 0.5550\n",
      "    Batch 110/264, Loss: 0.3646\n",
      "    Batch 120/264, Loss: 0.4463\n",
      "    Batch 130/264, Loss: 0.4168\n",
      "    Batch 140/264, Loss: 0.4891\n",
      "    Batch 150/264, Loss: 0.5670\n",
      "    Batch 160/264, Loss: 0.5081\n",
      "    Batch 170/264, Loss: 0.4439\n",
      "    Batch 180/264, Loss: 0.4151\n",
      "    Batch 190/264, Loss: 0.4853\n",
      "    Batch 200/264, Loss: 0.3769\n",
      "    Batch 210/264, Loss: 0.4821\n",
      "    Batch 220/264, Loss: 0.4509\n",
      "    Batch 230/264, Loss: 0.3155\n",
      "    Batch 240/264, Loss: 0.4417\n",
      "    Batch 250/264, Loss: 0.3583\n",
      "    Batch 260/264, Loss: 0.5027\n",
      "\n",
      "  Train Loss: 0.4493\n",
      "  Val Loss:   0.4305\n",
      "  Val Acc:    0.7530\n",
      "  Time:       17.7s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7139\n",
      "    WET: 0.8862\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 13/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.5345\n",
      "    Batch 20/264, Loss: 0.4167\n",
      "    Batch 30/264, Loss: 0.3727\n",
      "    Batch 40/264, Loss: 0.4392\n",
      "    Batch 50/264, Loss: 0.3394\n",
      "    Batch 60/264, Loss: 0.3993\n",
      "    Batch 70/264, Loss: 0.4920\n",
      "    Batch 80/264, Loss: 0.4293\n",
      "    Batch 90/264, Loss: 0.5732\n",
      "    Batch 100/264, Loss: 0.6027\n",
      "    Batch 110/264, Loss: 0.4236\n",
      "    Batch 120/264, Loss: 0.6735\n",
      "    Batch 130/264, Loss: 0.4202\n",
      "    Batch 140/264, Loss: 0.5335\n",
      "    Batch 150/264, Loss: 0.4195\n",
      "    Batch 160/264, Loss: 0.3377\n",
      "    Batch 170/264, Loss: 0.6289\n",
      "    Batch 180/264, Loss: 0.4340\n",
      "    Batch 190/264, Loss: 0.5100\n",
      "    Batch 200/264, Loss: 0.3335\n",
      "    Batch 210/264, Loss: 0.3671\n",
      "    Batch 220/264, Loss: 0.4878\n",
      "    Batch 230/264, Loss: 0.3945\n",
      "    Batch 240/264, Loss: 0.2989\n",
      "    Batch 250/264, Loss: 0.3781\n",
      "    Batch 260/264, Loss: 0.4099\n",
      "\n",
      "  Train Loss: 0.4371\n",
      "  Val Loss:   0.4248\n",
      "  Val Acc:    0.8171\n",
      "  Time:       17.6s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.8244\n",
      "    WET: 0.7920\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 14/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.3813\n",
      "    Batch 20/264, Loss: 0.3667\n",
      "    Batch 30/264, Loss: 0.5243\n",
      "    Batch 40/264, Loss: 0.3773\n",
      "    Batch 50/264, Loss: 0.6838\n",
      "    Batch 60/264, Loss: 0.5460\n",
      "    Batch 70/264, Loss: 0.4418\n",
      "    Batch 80/264, Loss: 0.4407\n",
      "    Batch 90/264, Loss: 0.4502\n",
      "    Batch 100/264, Loss: 0.3887\n",
      "    Batch 110/264, Loss: 0.6838\n",
      "    Batch 120/264, Loss: 0.3418\n",
      "    Batch 130/264, Loss: 0.3325\n",
      "    Batch 140/264, Loss: 0.4181\n",
      "    Batch 150/264, Loss: 0.4918\n",
      "    Batch 160/264, Loss: 0.7917\n",
      "    Batch 170/264, Loss: 0.3353\n",
      "    Batch 180/264, Loss: 0.5323\n",
      "    Batch 190/264, Loss: 0.4658\n",
      "    Batch 200/264, Loss: 0.5032\n",
      "    Batch 210/264, Loss: 0.3153\n",
      "    Batch 220/264, Loss: 0.5304\n",
      "    Batch 230/264, Loss: 0.4179\n",
      "    Batch 240/264, Loss: 0.3954\n",
      "    Batch 250/264, Loss: 0.5279\n",
      "    Batch 260/264, Loss: 0.3180\n",
      "\n",
      "  Train Loss: 0.4344\n",
      "  Val Loss:   0.4425\n",
      "  Val Acc:    0.7628\n",
      "  Time:       17.6s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7305\n",
      "    WET: 0.8729\n",
      "\n",
      "Epoch 15/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.4188\n",
      "    Batch 20/264, Loss: 0.4751\n",
      "    Batch 30/264, Loss: 0.4220\n",
      "    Batch 40/264, Loss: 0.9718\n",
      "    Batch 50/264, Loss: 0.3764\n",
      "    Batch 60/264, Loss: 0.3535\n",
      "    Batch 70/264, Loss: 0.2998\n",
      "    Batch 80/264, Loss: 0.3765\n",
      "    Batch 90/264, Loss: 0.3622\n",
      "    Batch 100/264, Loss: 0.3797\n",
      "    Batch 110/264, Loss: 0.4720\n",
      "    Batch 120/264, Loss: 0.3148\n",
      "    Batch 130/264, Loss: 0.5283\n",
      "    Batch 140/264, Loss: 0.4421\n",
      "    Batch 150/264, Loss: 0.2799\n",
      "    Batch 160/264, Loss: 0.4169\n",
      "    Batch 170/264, Loss: 0.4130\n",
      "    Batch 180/264, Loss: 0.3603\n",
      "    Batch 190/264, Loss: 0.4107\n",
      "    Batch 200/264, Loss: 0.3284\n",
      "    Batch 210/264, Loss: 0.4068\n",
      "    Batch 220/264, Loss: 0.4516\n",
      "    Batch 230/264, Loss: 0.5819\n",
      "    Batch 240/264, Loss: 0.6265\n",
      "    Batch 250/264, Loss: 0.4193\n",
      "    Batch 260/264, Loss: 0.4205\n",
      "\n",
      "  Train Loss: 0.4278\n",
      "  Val Loss:   0.4227\n",
      "  Val Acc:    0.7668\n",
      "  Time:       17.7s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7336\n",
      "    WET: 0.8797\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 16/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.3843\n",
      "    Batch 20/264, Loss: 0.4757\n",
      "    Batch 30/264, Loss: 0.5256\n",
      "    Batch 40/264, Loss: 0.4881\n",
      "    Batch 50/264, Loss: 0.4124\n",
      "    Batch 60/264, Loss: 0.4036\n",
      "    Batch 70/264, Loss: 0.4817\n",
      "    Batch 80/264, Loss: 0.3400\n",
      "    Batch 90/264, Loss: 0.3210\n",
      "    Batch 100/264, Loss: 0.2938\n",
      "    Batch 110/264, Loss: 0.5825\n",
      "    Batch 120/264, Loss: 0.4064\n",
      "    Batch 130/264, Loss: 0.4440\n",
      "    Batch 140/264, Loss: 0.5310\n",
      "    Batch 150/264, Loss: 0.5438\n",
      "    Batch 160/264, Loss: 0.3495\n",
      "    Batch 170/264, Loss: 0.4556\n",
      "    Batch 180/264, Loss: 0.3961\n",
      "    Batch 190/264, Loss: 0.4125\n",
      "    Batch 200/264, Loss: 0.3724\n",
      "    Batch 210/264, Loss: 0.4145\n",
      "    Batch 220/264, Loss: 0.4691\n",
      "    Batch 230/264, Loss: 0.3408\n",
      "    Batch 240/264, Loss: 0.3724\n",
      "    Batch 250/264, Loss: 0.3839\n",
      "    Batch 260/264, Loss: 0.3062\n",
      "\n",
      "  Train Loss: 0.4302\n",
      "  Val Loss:   0.4316\n",
      "  Val Acc:    0.8192\n",
      "  Time:       17.6s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.8315\n",
      "    WET: 0.7776\n",
      "\n",
      "Epoch 17/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.5962\n",
      "    Batch 20/264, Loss: 0.5594\n",
      "    Batch 30/264, Loss: 0.3313\n",
      "    Batch 40/264, Loss: 0.5828\n",
      "    Batch 50/264, Loss: 0.3809\n",
      "    Batch 60/264, Loss: 0.4103\n",
      "    Batch 70/264, Loss: 0.5054\n",
      "    Batch 80/264, Loss: 0.4679\n",
      "    Batch 90/264, Loss: 0.4603\n",
      "    Batch 100/264, Loss: 0.4926\n",
      "    Batch 110/264, Loss: 0.3056\n",
      "    Batch 120/264, Loss: 0.3520\n",
      "    Batch 130/264, Loss: 0.4226\n",
      "    Batch 140/264, Loss: 0.4584\n",
      "    Batch 150/264, Loss: 0.4564\n",
      "    Batch 160/264, Loss: 0.4570\n",
      "    Batch 170/264, Loss: 0.3362\n",
      "    Batch 180/264, Loss: 0.4618\n",
      "    Batch 190/264, Loss: 0.3813\n",
      "    Batch 200/264, Loss: 0.5987\n",
      "    Batch 210/264, Loss: 0.3363\n",
      "    Batch 220/264, Loss: 0.5375\n",
      "    Batch 230/264, Loss: 0.5406\n",
      "    Batch 240/264, Loss: 0.3980\n",
      "    Batch 250/264, Loss: 0.6456\n",
      "    Batch 260/264, Loss: 0.4049\n",
      "\n",
      "  Train Loss: 0.4349\n",
      "  Val Loss:   0.4334\n",
      "  Val Acc:    0.7921\n",
      "  Time:       17.9s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7806\n",
      "    WET: 0.8312\n",
      "\n",
      "Epoch 18/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.5239\n",
      "    Batch 20/264, Loss: 0.4597\n",
      "    Batch 30/264, Loss: 0.2988\n",
      "    Batch 40/264, Loss: 0.3136\n",
      "    Batch 50/264, Loss: 0.2861\n",
      "    Batch 60/264, Loss: 0.3482\n",
      "    Batch 70/264, Loss: 0.3523\n",
      "    Batch 80/264, Loss: 0.4646\n",
      "    Batch 90/264, Loss: 0.5124\n",
      "    Batch 100/264, Loss: 0.9798\n",
      "    Batch 110/264, Loss: 0.3773\n",
      "    Batch 120/264, Loss: 0.3527\n",
      "    Batch 130/264, Loss: 0.4691\n",
      "    Batch 140/264, Loss: 0.3630\n",
      "    Batch 150/264, Loss: 0.3546\n",
      "    Batch 160/264, Loss: 0.3106\n",
      "    Batch 170/264, Loss: 0.5606\n",
      "    Batch 180/264, Loss: 0.3971\n",
      "    Batch 190/264, Loss: 0.4203\n",
      "    Batch 200/264, Loss: 0.3252\n",
      "    Batch 210/264, Loss: 0.3255\n",
      "    Batch 220/264, Loss: 0.4630\n",
      "    Batch 230/264, Loss: 0.2660\n",
      "    Batch 240/264, Loss: 0.3900\n",
      "    Batch 250/264, Loss: 0.4967\n",
      "    Batch 260/264, Loss: 0.3897\n",
      "\n",
      "  Train Loss: 0.4327\n",
      "  Val Loss:   0.4448\n",
      "  Val Acc:    0.8167\n",
      "  Time:       17.9s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.8284\n",
      "    WET: 0.7769\n",
      "\n",
      "Epoch 19/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.4142\n",
      "    Batch 20/264, Loss: 0.3538\n",
      "    Batch 30/264, Loss: 0.2758\n",
      "    Batch 40/264, Loss: 0.3163\n",
      "    Batch 50/264, Loss: 0.3097\n",
      "    Batch 60/264, Loss: 0.4512\n",
      "    Batch 70/264, Loss: 0.3635\n",
      "    Batch 80/264, Loss: 0.4751\n",
      "    Batch 90/264, Loss: 0.3426\n",
      "    Batch 100/264, Loss: 0.3027\n",
      "    Batch 110/264, Loss: 0.3911\n",
      "    Batch 120/264, Loss: 0.4108\n",
      "    Batch 130/264, Loss: 0.2982\n",
      "    Batch 140/264, Loss: 0.3567\n",
      "    Batch 150/264, Loss: 0.5980\n",
      "    Batch 160/264, Loss: 0.4021\n",
      "    Batch 170/264, Loss: 0.3935\n",
      "    Batch 180/264, Loss: 0.4302\n",
      "    Batch 190/264, Loss: 0.4101\n",
      "    Batch 200/264, Loss: 0.4552\n",
      "    Batch 210/264, Loss: 0.4252\n",
      "    Batch 220/264, Loss: 0.4539\n",
      "    Batch 230/264, Loss: 0.4624\n",
      "    Batch 240/264, Loss: 0.4642\n",
      "    Batch 250/264, Loss: 0.4591\n",
      "    Batch 260/264, Loss: 0.2815\n",
      "\n",
      "  Train Loss: 0.4244\n",
      "  Val Loss:   0.4528\n",
      "  Val Acc:    0.7158\n",
      "  Time:       17.8s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6545\n",
      "    WET: 0.9248\n",
      "\n",
      "Epoch 20/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.3739\n",
      "    Batch 20/264, Loss: 0.4426\n",
      "    Batch 30/264, Loss: 0.3358\n",
      "    Batch 40/264, Loss: 0.6133\n",
      "    Batch 50/264, Loss: 0.3217\n",
      "    Batch 60/264, Loss: 0.4614\n",
      "    Batch 70/264, Loss: 0.4426\n",
      "    Batch 80/264, Loss: 0.4752\n",
      "    Batch 90/264, Loss: 0.3645\n",
      "    Batch 100/264, Loss: 0.3401\n",
      "    Batch 110/264, Loss: 0.3193\n",
      "    Batch 120/264, Loss: 0.3995\n",
      "    Batch 130/264, Loss: 0.3990\n",
      "    Batch 140/264, Loss: 0.4094\n",
      "    Batch 150/264, Loss: 0.4203\n",
      "    Batch 160/264, Loss: 0.2385\n",
      "    Batch 170/264, Loss: 0.3618\n",
      "    Batch 180/264, Loss: 0.5114\n",
      "    Batch 190/264, Loss: 0.5010\n",
      "    Batch 200/264, Loss: 0.5525\n",
      "    Batch 210/264, Loss: 0.4905\n",
      "    Batch 220/264, Loss: 0.4482\n",
      "    Batch 230/264, Loss: 0.3768\n",
      "    Batch 240/264, Loss: 0.4387\n",
      "    Batch 250/264, Loss: 0.3655\n",
      "    Batch 260/264, Loss: 0.3752\n",
      "\n",
      "  Train Loss: 0.4184\n",
      "  Val Loss:   0.4667\n",
      "  Val Acc:    0.8066\n",
      "  Time:       17.7s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.8106\n",
      "    WET: 0.7933\n",
      "\n",
      "Epoch 21/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.2869\n",
      "    Batch 20/264, Loss: 0.3846\n",
      "    Batch 30/264, Loss: 0.2457\n",
      "    Batch 40/264, Loss: 0.4767\n",
      "    Batch 50/264, Loss: 0.5950\n",
      "    Batch 60/264, Loss: 0.3264\n",
      "    Batch 70/264, Loss: 0.4222\n",
      "    Batch 80/264, Loss: 0.3861\n",
      "    Batch 90/264, Loss: 0.3658\n",
      "    Batch 100/264, Loss: 0.4135\n",
      "    Batch 110/264, Loss: 0.3349\n",
      "    Batch 120/264, Loss: 0.4411\n",
      "    Batch 130/264, Loss: 0.3242\n",
      "    Batch 140/264, Loss: 0.3630\n",
      "    Batch 150/264, Loss: 0.3524\n",
      "    Batch 160/264, Loss: 0.3125\n",
      "    Batch 170/264, Loss: 0.5487\n",
      "    Batch 180/264, Loss: 0.5538\n",
      "    Batch 190/264, Loss: 0.3264\n",
      "    Batch 200/264, Loss: 0.3238\n",
      "    Batch 210/264, Loss: 0.5036\n",
      "    Batch 220/264, Loss: 0.5914\n",
      "    Batch 230/264, Loss: 0.4361\n",
      "    Batch 240/264, Loss: 0.4133\n",
      "    Batch 250/264, Loss: 0.4631\n",
      "    Batch 260/264, Loss: 0.6293\n",
      "\n",
      "  Train Loss: 0.4168\n",
      "  Val Loss:   0.4191\n",
      "  Val Acc:    0.7644\n",
      "  Time:       18.0s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7280\n",
      "    WET: 0.8886\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 22/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.3175\n",
      "    Batch 20/264, Loss: 0.3209\n",
      "    Batch 30/264, Loss: 0.4357\n",
      "    Batch 40/264, Loss: 0.3436\n",
      "    Batch 50/264, Loss: 0.4334\n",
      "    Batch 60/264, Loss: 0.5802\n",
      "    Batch 70/264, Loss: 0.4222\n",
      "    Batch 80/264, Loss: 0.4518\n",
      "    Batch 90/264, Loss: 0.3947\n",
      "    Batch 100/264, Loss: 0.3783\n",
      "    Batch 110/264, Loss: 0.3102\n",
      "    Batch 120/264, Loss: 0.3535\n",
      "    Batch 130/264, Loss: 0.3686\n",
      "    Batch 140/264, Loss: 0.3564\n",
      "    Batch 150/264, Loss: 0.4844\n",
      "    Batch 160/264, Loss: 0.5808\n",
      "    Batch 170/264, Loss: 0.3027\n",
      "    Batch 180/264, Loss: 0.3913\n",
      "    Batch 190/264, Loss: 0.6365\n",
      "    Batch 200/264, Loss: 0.4773\n",
      "    Batch 210/264, Loss: 0.3462\n",
      "    Batch 220/264, Loss: 0.5288\n",
      "    Batch 230/264, Loss: 0.3635\n",
      "    Batch 240/264, Loss: 0.4965\n",
      "    Batch 250/264, Loss: 0.4139\n",
      "    Batch 260/264, Loss: 0.4585\n",
      "\n",
      "  Train Loss: 0.4131\n",
      "  Val Loss:   0.4404\n",
      "  Val Acc:    0.8172\n",
      "  Time:       17.6s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.8299\n",
      "    WET: 0.7739\n",
      "\n",
      "Epoch 23/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.4734\n",
      "    Batch 20/264, Loss: 0.4055\n",
      "    Batch 30/264, Loss: 0.2305\n",
      "    Batch 40/264, Loss: 0.2676\n",
      "    Batch 50/264, Loss: 0.3715\n",
      "    Batch 60/264, Loss: 0.4019\n",
      "    Batch 70/264, Loss: 0.3243\n",
      "    Batch 80/264, Loss: 0.2968\n",
      "    Batch 90/264, Loss: 0.2795\n",
      "    Batch 100/264, Loss: 0.3444\n",
      "    Batch 110/264, Loss: 0.4182\n",
      "    Batch 120/264, Loss: 0.4034\n",
      "    Batch 130/264, Loss: 0.4088\n",
      "    Batch 140/264, Loss: 0.3237\n",
      "    Batch 150/264, Loss: 0.4373\n",
      "    Batch 160/264, Loss: 0.4472\n",
      "    Batch 170/264, Loss: 0.5574\n",
      "    Batch 180/264, Loss: 0.2643\n",
      "    Batch 190/264, Loss: 0.4950\n",
      "    Batch 200/264, Loss: 0.7177\n",
      "    Batch 210/264, Loss: 0.3715\n",
      "    Batch 220/264, Loss: 0.3803\n",
      "    Batch 230/264, Loss: 0.3603\n",
      "    Batch 240/264, Loss: 0.5376\n",
      "    Batch 250/264, Loss: 0.4781\n",
      "    Batch 260/264, Loss: 0.3057\n",
      "\n",
      "  Train Loss: 0.4173\n",
      "  Val Loss:   0.4430\n",
      "  Val Acc:    0.7518\n",
      "  Time:       17.6s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7071\n",
      "    WET: 0.9041\n",
      "\n",
      "Epoch 24/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.2630\n",
      "    Batch 20/264, Loss: 0.4372\n",
      "    Batch 30/264, Loss: 0.4915\n",
      "    Batch 40/264, Loss: 0.2771\n",
      "    Batch 50/264, Loss: 0.3899\n",
      "    Batch 60/264, Loss: 0.4536\n",
      "    Batch 70/264, Loss: 0.2991\n",
      "    Batch 80/264, Loss: 0.5419\n",
      "    Batch 90/264, Loss: 0.2512\n",
      "    Batch 100/264, Loss: 0.4442\n",
      "    Batch 110/264, Loss: 0.6099\n",
      "    Batch 120/264, Loss: 0.4139\n",
      "    Batch 130/264, Loss: 0.3854\n",
      "    Batch 140/264, Loss: 0.3668\n",
      "    Batch 150/264, Loss: 0.4429\n",
      "    Batch 160/264, Loss: 0.2427\n",
      "    Batch 170/264, Loss: 0.3688\n",
      "    Batch 180/264, Loss: 0.3453\n",
      "    Batch 190/264, Loss: 0.6085\n",
      "    Batch 200/264, Loss: 0.5456\n",
      "    Batch 210/264, Loss: 0.3190\n",
      "    Batch 220/264, Loss: 0.3705\n",
      "    Batch 230/264, Loss: 0.4556\n",
      "    Batch 240/264, Loss: 0.3958\n",
      "    Batch 250/264, Loss: 0.3676\n",
      "    Batch 260/264, Loss: 0.3901\n",
      "\n",
      "  Train Loss: 0.4202\n",
      "  Val Loss:   0.4483\n",
      "  Val Acc:    0.8014\n",
      "  Time:       17.7s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7966\n",
      "    WET: 0.8179\n",
      "\n",
      "Epoch 25/25\n",
      "----------------------------------------\n",
      "    Batch 10/264, Loss: 0.4513\n",
      "    Batch 20/264, Loss: 0.3041\n",
      "    Batch 30/264, Loss: 0.5405\n",
      "    Batch 40/264, Loss: 0.4255\n",
      "    Batch 50/264, Loss: 0.3808\n",
      "    Batch 60/264, Loss: 0.4176\n",
      "    Batch 70/264, Loss: 0.3375\n",
      "    Batch 80/264, Loss: 0.2970\n",
      "    Batch 90/264, Loss: 0.2481\n",
      "    Batch 100/264, Loss: 0.4120\n",
      "    Batch 110/264, Loss: 0.4843\n",
      "    Batch 120/264, Loss: 0.4566\n",
      "    Batch 130/264, Loss: 0.3732\n",
      "    Batch 140/264, Loss: 0.4164\n",
      "    Batch 150/264, Loss: 0.2827\n",
      "    Batch 160/264, Loss: 0.3968\n",
      "    Batch 170/264, Loss: 0.4208\n",
      "    Batch 180/264, Loss: 0.4091\n",
      "    Batch 190/264, Loss: 0.4088\n",
      "    Batch 200/264, Loss: 0.4827\n",
      "    Batch 210/264, Loss: 0.5101\n",
      "    Batch 220/264, Loss: 0.3796\n",
      "    Batch 230/264, Loss: 0.3348\n",
      "    Batch 240/264, Loss: 0.4981\n",
      "    Batch 250/264, Loss: 0.3545\n",
      "    Batch 260/264, Loss: 0.3609\n",
      "\n",
      "  Train Loss: 0.4091\n",
      "  Val Loss:   0.4122\n",
      "  Val Acc:    0.8130\n",
      "  Time:       17.6s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.8165\n",
      "    WET: 0.8009\n",
      "  [Saved new best model]\n",
      "\n",
      "============================================================\n",
      "Training complete!\n",
      "Best validation loss: 0.4122\n",
      "Models saved to: Models\n"
     ]
    }
   ],
   "source": [
    "def compute_class_weights(data_dir, cluster_id, huc_id, class_names):\n",
    "    \"\"\"Compute class weights from training data using inverse frequency.\"\"\"\n",
    "    files = find_patch_files(data_dir, cluster_id, huc_id)\n",
    "    \n",
    "    print(\"Loading y_train files for class weight computation...\")\n",
    "    y_train_list = [np.load(f) for f in files['y_train']]\n",
    "    y_train = np.concatenate(y_train_list, axis=0)\n",
    "    print(f\"  Combined y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    # Count pixels per class\n",
    "    classes, counts = np.unique(y_train, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    \n",
    "    # Compute inverse frequency weights\n",
    "    frequencies = counts / total\n",
    "    weights = 1.0 / frequencies\n",
    "    weights = weights / weights.min()  # Normalize so smallest weight is 1.0\n",
    "    \n",
    "    print(\"\\nClass distribution and weights:\")\n",
    "    for c, count, w in zip(classes, counts, weights):\n",
    "        pct = count / total * 100\n",
    "        print(f\"  {class_names[c]}: {count:,} pixels ({pct:.2f}%) -> weight: {w:.2f}\")\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Device selection\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                          \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # === COMPUTE CLASS WEIGHTS ===\n",
    "    class_weights = compute_class_weights(\n",
    "        data_dir, cluster_id, huc_id, metadata[\"class_names\"]\n",
    "    )\n",
    "\n",
    "    # === CREATE MODEL ===\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = UNet(\n",
    "        in_channels=metadata[\"in_channels\"],\n",
    "        num_classes=metadata[\"num_classes\"],\n",
    "        base_filters=base_filters\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "    # === LOSS AND OPTIMIZER ===\n",
    "    class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # === TRAINING LOOP ===\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc, class_acc = validate(model, val_loader, criterion, device, metadata)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        # Log results\n",
    "        print(f\"\\n  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "        print(f\"  Val Acc:    {val_acc:.4f}\")\n",
    "        print(f\"  Time:       {epoch_time:.1f}s\")\n",
    "        print(\"  Per-class accuracy:\")\n",
    "        for name, acc in class_acc.items():\n",
    "            print(f\"    {name}: {acc:.4f}\")\n",
    "\n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'metadata': metadata,\n",
    "                'config': {\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'huc_id': huc_id,\n",
    "                    'base_filters': base_filters,\n",
    "                    'learning_rate': learning_rate,\n",
    "                }\n",
    "            }, output_dir / \"best_model.pth\")\n",
    "            print(\"  [Saved new best model]\")\n",
    "\n",
    "    # === SAVE FINAL MODEL AND HISTORY ===\n",
    "    torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'metadata': metadata,\n",
    "        'config': {\n",
    "            'cluster_id': cluster_id,\n",
    "            'huc_id': huc_id,\n",
    "            'base_filters': base_filters,\n",
    "            'learning_rate': learning_rate,\n",
    "        }\n",
    "    }, output_dir / \"final_model.pth\")\n",
    "\n",
    "    np.save(output_dir / \"training_history.npy\", history)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Models saved to: {output_dir}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# Run training (comment out to prevent execution)\n",
    "history = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ddb62c-fe9c-47ad-b686-6958e348857e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove"
    ]
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script Python_Code_Analysis/DL_Implement/NYS_06_train.ipynb --TagRemovePreprocessor.remove_cell_tags='{\"remove\"}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wetland-cnn",
   "language": "python",
   "name": "wetland-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
