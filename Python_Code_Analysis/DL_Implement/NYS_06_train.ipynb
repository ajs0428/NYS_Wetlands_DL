{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5c3bf5-9201-4c7f-b5b6-7622df105d1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/Anthony/Data and Analysis Local/NYS_Wetlands_DL\n",
      "\n",
      "Data Configuration:\n",
      "  data_dir: Data/Patches_v2\n",
      "  cluster_id: 208\n",
      "  huc_id: All HUCs in cluster\n",
      "\n",
      "Training Configuration:\n",
      "  num_epochs: 10\n",
      "  batch_size: 10\n",
      "  learning_rate: 0.001\n",
      "  base_filters: 32\n",
      "  output_dir: Models\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "workdir = Path(\"/Users/Anthony/Data and Analysis Local/NYS_Wetlands_DL/\")\n",
    "os.chdir(workdir)\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Must match the output from NYS_03_create_patches_v2.ipynb\n",
    "data_dir = Path(\"Data/Patches_v2\")\n",
    "cluster_id = 208  # Cluster to load, or None for legacy files\n",
    "huc_id = None     # Specific HUC to load, or None to combine all HUCs in cluster\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "base_filters = 32  # U-Net base filter count\n",
    "\n",
    "# Output directory for models\n",
    "output_dir = Path(\"Models\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\nData Configuration:\")\n",
    "print(f\"  data_dir: {data_dir}\")\n",
    "print(f\"  cluster_id: {cluster_id}\")\n",
    "print(f\"  huc_id: {huc_id or 'All HUCs in cluster'}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  num_epochs: {num_epochs}\")\n",
    "print(f\"  batch_size: {batch_size}\")\n",
    "print(f\"  learning_rate: {learning_rate}\")\n",
    "print(f\"  base_filters: {base_filters}\")\n",
    "print(f\"  output_dir: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b3ef68c-2b57-4ed7-b9cc-8d0fb0fe8a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('Models')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b38b106-673a-4eb4-8628-95db182d4e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Found 9 training file(s)\n",
      "Found 9 validation file(s)\n",
      "\n",
      "Dataset Summary:\n",
      "  Training batches: 253\n",
      "  Validation batches: 64\n",
      "  in_channels: 11\n",
      "  num_classes: 5\n",
      "  band_names: ['r', 'g', 'b', 'nir', 'ndvi', 'ndwi', 'dem', 'chm', 'slope_5m', 'TPI_5m', 'Geomorph_5m']\n",
      "  HUCs included: ['metadata', '041402011002', 'metadata', '041402011004', 'metadata', 'metadata', 'metadata', 'metadata', 'metadata', 'metadata', 'metadata']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Add script directory to Python path\n",
    "script_dir = Path(\"Python_Code_Analysis/DL_Implement/\")\n",
    "sys.path.insert(0, str(script_dir))\n",
    "\n",
    "# Import modules\n",
    "from NYS_04_dataset import get_dataloaders, find_patch_files, load_and_merge_metadata\n",
    "from NYS_05_unet_model import UNet\n",
    "\n",
    "# === LOAD DATA ===\n",
    "print(\"Loading data...\")\n",
    "train_loader, val_loader, metadata = get_dataloaders(\n",
    "    data_dir, \n",
    "    cluster_id=cluster_id, \n",
    "    huc_id=huc_id, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  in_channels: {metadata['in_channels']}\")\n",
    "print(f\"  num_classes: {metadata['num_classes']}\")\n",
    "print(f\"  band_names: {metadata['band_names']}\")\n",
    "if \"hucs_included\" in metadata:\n",
    "    print(f\"  HUCs included: {metadata['hucs_included']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c47603f5-d35e-44d1-89fc-07c44c8787df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove"
    ]
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch and return average loss.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Progress update every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"    Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, metadata):\n",
    "    \"\"\"Validate and return loss plus per-class accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    num_classes = metadata[\"num_classes\"]\n",
    "    class_names = metadata[\"class_names\"]\n",
    "\n",
    "    # Track correct predictions per class\n",
    "    correct_per_class = torch.zeros(num_classes)\n",
    "    total_per_class = torch.zeros(num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Per-class accuracy\n",
    "            for c in range(num_classes):\n",
    "                mask = (y == c)\n",
    "                total_per_class[c] += mask.sum().item()\n",
    "                correct_per_class[c] += ((preds == c) & mask).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "\n",
    "    # Calculate per-class accuracy\n",
    "    class_acc = {}\n",
    "    for c in range(num_classes):\n",
    "        if total_per_class[c] > 0:\n",
    "            class_acc[class_names[c]] = correct_per_class[c] / total_per_class[c]\n",
    "        else:\n",
    "            class_acc[class_names[c]] = 0.0\n",
    "\n",
    "    # Overall accuracy\n",
    "    overall_acc = correct_per_class.sum() / total_per_class.sum()\n",
    "\n",
    "    return avg_loss, overall_acc.item(), class_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8691be53-c8a1-4658-a540-61b84a1765ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading y_train files for class weight computation...\n",
      "  Combined y_train shape: (2524, 256, 256)\n",
      "\n",
      "Class distribution and weights:\n",
      "  Background: 144,829,890 pixels (87.56%) -> weight: 1.00\n",
      "  EMW: 4,826,975 pixels (2.92%) -> weight: 30.00\n",
      "  FSW: 8,329,397 pixels (5.04%) -> weight: 17.39\n",
      "  SSW: 5,116,591 pixels (3.09%) -> weight: 28.31\n",
      "  OWW: 2,310,011 pixels (1.40%) -> weight: 62.70\n",
      "\n",
      "Initializing model...\n",
      "Total parameters: 7,768,421\n",
      "\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "Epoch 1/10\n",
      "----------------------------------------\n",
      "    Batch 10/253, Loss: 1.5199\n",
      "    Batch 20/253, Loss: 1.3826\n",
      "    Batch 30/253, Loss: 1.5391\n",
      "    Batch 40/253, Loss: 1.4731\n",
      "    Batch 50/253, Loss: 1.3011\n",
      "    Batch 60/253, Loss: 1.5525\n",
      "    Batch 70/253, Loss: 1.4379\n",
      "    Batch 80/253, Loss: 1.3097\n",
      "    Batch 90/253, Loss: 1.4480\n",
      "    Batch 100/253, Loss: 1.2421\n",
      "    Batch 110/253, Loss: 1.2923\n",
      "    Batch 120/253, Loss: 1.2947\n",
      "    Batch 130/253, Loss: 1.3069\n",
      "    Batch 140/253, Loss: 1.1506\n",
      "    Batch 150/253, Loss: 1.0566\n",
      "    Batch 160/253, Loss: 1.2037\n",
      "    Batch 170/253, Loss: 1.2534\n",
      "    Batch 180/253, Loss: 1.2811\n",
      "    Batch 190/253, Loss: 1.2821\n",
      "    Batch 200/253, Loss: 1.1261\n",
      "    Batch 210/253, Loss: 1.4016\n",
      "    Batch 220/253, Loss: 1.1051\n",
      "    Batch 230/253, Loss: 1.2997\n",
      "    Batch 240/253, Loss: 1.4158\n",
      "    Batch 250/253, Loss: 1.4518\n",
      "\n",
      "  Train Loss: 1.3309\n",
      "  Val Loss:   1.1619\n",
      "  Val Acc:    0.6595\n",
      "  Time:       74.5s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6913\n",
      "    EMW: 0.5889\n",
      "    FSW: 0.5626\n",
      "    SSW: 0.0513\n",
      "    OWW: 0.7590\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 2/10\n",
      "----------------------------------------\n",
      "    Batch 10/253, Loss: 1.6229\n",
      "    Batch 20/253, Loss: 1.1186\n",
      "    Batch 30/253, Loss: 1.2843\n",
      "    Batch 40/253, Loss: 1.1766\n",
      "    Batch 50/253, Loss: 0.9490\n",
      "    Batch 60/253, Loss: 1.4418\n",
      "    Batch 70/253, Loss: 1.0962\n",
      "    Batch 80/253, Loss: 1.3003\n",
      "    Batch 90/253, Loss: 1.2495\n",
      "    Batch 100/253, Loss: 1.6891\n",
      "    Batch 110/253, Loss: 1.5352\n",
      "    Batch 120/253, Loss: 1.2116\n",
      "    Batch 130/253, Loss: 1.1187\n",
      "    Batch 140/253, Loss: 1.2853\n",
      "    Batch 150/253, Loss: 0.9011\n",
      "    Batch 160/253, Loss: 1.2229\n",
      "    Batch 170/253, Loss: 1.3470\n",
      "    Batch 180/253, Loss: 1.2858\n",
      "    Batch 190/253, Loss: 1.0479\n",
      "    Batch 200/253, Loss: 1.1545\n",
      "    Batch 210/253, Loss: 1.5544\n",
      "    Batch 220/253, Loss: 1.0288\n",
      "    Batch 230/253, Loss: 1.2421\n",
      "    Batch 240/253, Loss: 1.5535\n",
      "    Batch 250/253, Loss: 1.3087\n",
      "\n",
      "  Train Loss: 1.2443\n",
      "  Val Loss:   1.2186\n",
      "  Val Acc:    0.5503\n",
      "  Time:       64.9s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.5581\n",
      "    EMW: 0.4394\n",
      "    FSW: 0.5370\n",
      "    SSW: 0.5048\n",
      "    OWW: 0.5454\n",
      "\n",
      "Epoch 3/10\n",
      "----------------------------------------\n",
      "    Batch 10/253, Loss: 1.2704\n",
      "    Batch 20/253, Loss: 1.1252\n",
      "    Batch 30/253, Loss: 0.9409\n",
      "    Batch 40/253, Loss: 1.1399\n",
      "    Batch 50/253, Loss: 1.0152\n",
      "    Batch 60/253, Loss: 0.9167\n",
      "    Batch 70/253, Loss: 1.0238\n",
      "    Batch 80/253, Loss: 1.3041\n",
      "    Batch 90/253, Loss: 1.1505\n",
      "    Batch 100/253, Loss: 1.3009\n",
      "    Batch 110/253, Loss: 1.2245\n",
      "    Batch 120/253, Loss: 1.1211\n",
      "    Batch 130/253, Loss: 1.2543\n",
      "    Batch 140/253, Loss: 1.0565\n",
      "    Batch 150/253, Loss: 1.0605\n",
      "    Batch 160/253, Loss: 1.1362\n",
      "    Batch 170/253, Loss: 1.3585\n",
      "    Batch 180/253, Loss: 1.1894\n",
      "    Batch 190/253, Loss: 1.4047\n",
      "    Batch 200/253, Loss: 1.0099\n",
      "    Batch 210/253, Loss: 1.2779\n",
      "    Batch 220/253, Loss: 1.0342\n",
      "    Batch 230/253, Loss: 1.2055\n",
      "    Batch 240/253, Loss: 1.4709\n",
      "    Batch 250/253, Loss: 1.0906\n",
      "\n",
      "  Train Loss: 1.1922\n",
      "  Val Loss:   1.0951\n",
      "  Val Acc:    0.6819\n",
      "  Time:       64.9s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7163\n",
      "    EMW: 0.5741\n",
      "    FSW: 0.5541\n",
      "    SSW: 0.1062\n",
      "    OWW: 0.7840\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 4/10\n",
      "----------------------------------------\n",
      "    Batch 10/253, Loss: 0.8171\n",
      "    Batch 20/253, Loss: 0.9490\n",
      "    Batch 30/253, Loss: 1.3409\n",
      "    Batch 40/253, Loss: 1.0613\n",
      "    Batch 50/253, Loss: 1.1813\n",
      "    Batch 60/253, Loss: 1.3487\n",
      "    Batch 70/253, Loss: 1.1500\n",
      "    Batch 80/253, Loss: 1.1989\n",
      "    Batch 90/253, Loss: 1.2760\n",
      "    Batch 100/253, Loss: 1.1680\n",
      "    Batch 110/253, Loss: 1.3254\n",
      "    Batch 120/253, Loss: 1.2664\n",
      "    Batch 130/253, Loss: 1.1797\n",
      "    Batch 140/253, Loss: 1.9347\n",
      "    Batch 150/253, Loss: 1.0452\n",
      "    Batch 160/253, Loss: 1.0820\n",
      "    Batch 170/253, Loss: 1.3928\n",
      "    Batch 180/253, Loss: 1.2156\n",
      "    Batch 190/253, Loss: 1.7602\n",
      "    Batch 200/253, Loss: 1.2580\n",
      "    Batch 210/253, Loss: 1.1267\n",
      "    Batch 220/253, Loss: 0.9675\n",
      "    Batch 230/253, Loss: 1.0500\n",
      "    Batch 240/253, Loss: 1.1619\n",
      "    Batch 250/253, Loss: 0.9501\n",
      "\n",
      "  Train Loss: 1.1697\n",
      "  Val Loss:   1.1512\n",
      "  Val Acc:    0.3939\n",
      "  Time:       64.9s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.3617\n",
      "    EMW: 0.3792\n",
      "    FSW: 0.7510\n",
      "    SSW: 0.3389\n",
      "    OWW: 0.7623\n",
      "\n",
      "Epoch 5/10\n",
      "----------------------------------------\n",
      "    Batch 10/253, Loss: 1.2972\n",
      "    Batch 20/253, Loss: 0.9498\n",
      "    Batch 30/253, Loss: 1.2023\n",
      "    Batch 40/253, Loss: 1.1231\n",
      "    Batch 50/253, Loss: 1.1581\n",
      "    Batch 60/253, Loss: 1.0345\n",
      "    Batch 70/253, Loss: 1.2384\n",
      "    Batch 80/253, Loss: 1.2454\n",
      "    Batch 90/253, Loss: 1.4579\n",
      "    Batch 100/253, Loss: 1.0999\n",
      "    Batch 110/253, Loss: 0.9549\n",
      "    Batch 120/253, Loss: 0.9486\n",
      "    Batch 130/253, Loss: 1.1673\n",
      "    Batch 140/253, Loss: 1.3735\n",
      "    Batch 150/253, Loss: 1.1765\n",
      "    Batch 160/253, Loss: 1.0678\n",
      "    Batch 170/253, Loss: 1.3293\n",
      "    Batch 180/253, Loss: 1.1088\n",
      "    Batch 190/253, Loss: 0.9259\n",
      "    Batch 200/253, Loss: 0.8848\n",
      "    Batch 210/253, Loss: 1.0230\n",
      "    Batch 220/253, Loss: 1.1729\n",
      "    Batch 230/253, Loss: 0.8564\n",
      "    Batch 240/253, Loss: 0.9690\n",
      "    Batch 250/253, Loss: 1.0944\n",
      "\n",
      "  Train Loss: 1.1438\n",
      "  Val Loss:   1.0485\n",
      "  Val Acc:    0.6288\n",
      "  Time:       64.7s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6420\n",
      "    EMW: 0.5306\n",
      "    FSW: 0.7475\n",
      "    SSW: 0.0821\n",
      "    OWW: 0.8066\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 6/10\n",
      "----------------------------------------\n",
      "    Batch 10/253, Loss: 1.3957\n",
      "    Batch 20/253, Loss: 1.2741\n",
      "    Batch 30/253, Loss: 0.9270\n",
      "    Batch 40/253, Loss: 1.2685\n",
      "    Batch 50/253, Loss: 1.5120\n",
      "    Batch 60/253, Loss: 0.9464\n",
      "    Batch 70/253, Loss: 1.3910\n",
      "    Batch 80/253, Loss: 1.2199\n",
      "    Batch 90/253, Loss: 1.1589\n",
      "    Batch 100/253, Loss: 1.2089\n",
      "    Batch 110/253, Loss: 0.9452\n",
      "    Batch 120/253, Loss: 1.2181\n",
      "    Batch 130/253, Loss: 1.2486\n",
      "    Batch 140/253, Loss: 1.1091\n",
      "    Batch 150/253, Loss: 1.1377\n",
      "    Batch 160/253, Loss: 0.9977\n",
      "    Batch 170/253, Loss: 0.9998\n",
      "    Batch 180/253, Loss: 1.0856\n",
      "    Batch 190/253, Loss: 1.5346\n",
      "    Batch 200/253, Loss: 0.9735\n",
      "    Batch 210/253, Loss: 1.0283\n",
      "    Batch 220/253, Loss: 0.9201\n",
      "    Batch 230/253, Loss: 1.0665\n",
      "    Batch 240/253, Loss: 1.0858\n",
      "    Batch 250/253, Loss: 1.0059\n",
      "\n",
      "  Train Loss: 1.1179\n",
      "  Val Loss:   1.0229\n",
      "  Val Acc:    0.6957\n",
      "  Time:       65.5s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7210\n",
      "    EMW: 0.5504\n",
      "    FSW: 0.7107\n",
      "    SSW: 0.1495\n",
      "    OWW: 0.7706\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 7/10\n",
      "----------------------------------------\n",
      "    Batch 10/253, Loss: 1.7478\n",
      "    Batch 20/253, Loss: 1.3259\n",
      "    Batch 30/253, Loss: 1.2835\n",
      "    Batch 40/253, Loss: 1.2330\n",
      "    Batch 50/253, Loss: 1.1444\n",
      "    Batch 60/253, Loss: 1.0010\n",
      "    Batch 70/253, Loss: 1.0426\n",
      "    Batch 80/253, Loss: 0.9719\n",
      "    Batch 90/253, Loss: 1.1765\n",
      "    Batch 100/253, Loss: 1.0747\n",
      "    Batch 110/253, Loss: 1.0479\n",
      "    Batch 120/253, Loss: 1.1743\n",
      "    Batch 130/253, Loss: 1.1754\n",
      "    Batch 140/253, Loss: 0.9578\n",
      "    Batch 150/253, Loss: 1.3264\n",
      "    Batch 160/253, Loss: 1.3315\n",
      "    Batch 170/253, Loss: 0.9052\n",
      "    Batch 180/253, Loss: 1.2461\n",
      "    Batch 190/253, Loss: 0.9314\n",
      "    Batch 200/253, Loss: 0.8507\n",
      "    Batch 210/253, Loss: 1.0420\n",
      "    Batch 220/253, Loss: 1.1856\n",
      "    Batch 230/253, Loss: 0.9163\n",
      "    Batch 240/253, Loss: 0.8429\n",
      "    Batch 250/253, Loss: 1.1283\n",
      "\n",
      "  Train Loss: 1.1122\n",
      "  Val Loss:   1.0445\n",
      "  Val Acc:    0.7049\n",
      "  Time:       66.7s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7396\n",
      "    EMW: 0.3952\n",
      "    FSW: 0.5425\n",
      "    SSW: 0.3712\n",
      "    OWW: 0.8579\n",
      "\n",
      "Epoch 8/10\n",
      "----------------------------------------\n",
      "    Batch 10/253, Loss: 0.7839\n",
      "    Batch 20/253, Loss: 0.8928\n",
      "    Batch 30/253, Loss: 1.1157\n",
      "    Batch 40/253, Loss: 1.0460\n",
      "    Batch 50/253, Loss: 0.9429\n",
      "    Batch 60/253, Loss: 1.1490\n",
      "    Batch 70/253, Loss: 1.3510\n",
      "    Batch 80/253, Loss: 1.1074\n",
      "    Batch 90/253, Loss: 1.0917\n",
      "    Batch 100/253, Loss: 1.1752\n",
      "    Batch 110/253, Loss: 0.9280\n",
      "    Batch 120/253, Loss: 1.1861\n",
      "    Batch 130/253, Loss: 1.0733\n",
      "    Batch 140/253, Loss: 1.1683\n",
      "    Batch 150/253, Loss: 0.9186\n",
      "    Batch 160/253, Loss: 1.2617\n",
      "    Batch 170/253, Loss: 1.1235\n",
      "    Batch 180/253, Loss: 1.2859\n",
      "    Batch 190/253, Loss: 1.3419\n",
      "    Batch 200/253, Loss: 1.1106\n",
      "    Batch 210/253, Loss: 0.9671\n",
      "    Batch 220/253, Loss: 1.2734\n",
      "    Batch 230/253, Loss: 1.0435\n",
      "    Batch 240/253, Loss: 0.9794\n",
      "    Batch 250/253, Loss: 1.1527\n",
      "\n",
      "  Train Loss: 1.1046\n",
      "  Val Loss:   1.0265\n",
      "  Val Acc:    0.5680\n",
      "  Time:       66.8s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.5691\n",
      "    EMW: 0.4441\n",
      "    FSW: 0.6717\n",
      "    SSW: 0.2785\n",
      "    OWW: 0.8975\n",
      "\n",
      "Epoch 9/10\n",
      "----------------------------------------\n",
      "    Batch 10/253, Loss: 1.1464\n",
      "    Batch 20/253, Loss: 1.2121\n",
      "    Batch 30/253, Loss: 0.6659\n",
      "    Batch 40/253, Loss: 0.9857\n",
      "    Batch 50/253, Loss: 1.2153\n",
      "    Batch 60/253, Loss: 1.1036\n",
      "    Batch 70/253, Loss: 1.1810\n",
      "    Batch 80/253, Loss: 0.5590\n",
      "    Batch 90/253, Loss: 0.9806\n",
      "    Batch 100/253, Loss: 1.2720\n",
      "    Batch 110/253, Loss: 0.6486\n",
      "    Batch 120/253, Loss: 0.7197\n",
      "    Batch 130/253, Loss: 1.1659\n",
      "    Batch 140/253, Loss: 1.0890\n",
      "    Batch 150/253, Loss: 0.7778\n",
      "    Batch 160/253, Loss: 1.1623\n",
      "    Batch 170/253, Loss: 1.0393\n",
      "    Batch 180/253, Loss: 1.4469\n",
      "    Batch 190/253, Loss: 1.2206\n",
      "    Batch 200/253, Loss: 1.5173\n",
      "    Batch 210/253, Loss: 1.0523\n",
      "    Batch 220/253, Loss: 0.9090\n",
      "    Batch 230/253, Loss: 1.0068\n",
      "    Batch 240/253, Loss: 0.9478\n",
      "    Batch 250/253, Loss: 1.1508\n",
      "\n",
      "  Train Loss: 1.0871\n",
      "  Val Loss:   1.0401\n",
      "  Val Acc:    0.6161\n",
      "  Time:       64.9s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6251\n",
      "    EMW: 0.3894\n",
      "    FSW: 0.7752\n",
      "    SSW: 0.2182\n",
      "    OWW: 0.8298\n",
      "\n",
      "Epoch 10/10\n",
      "----------------------------------------\n",
      "    Batch 10/253, Loss: 1.0060\n",
      "    Batch 20/253, Loss: 1.2011\n",
      "    Batch 30/253, Loss: 1.0131\n",
      "    Batch 40/253, Loss: 0.7613\n",
      "    Batch 50/253, Loss: 1.1015\n",
      "    Batch 60/253, Loss: 0.9155\n",
      "    Batch 70/253, Loss: 1.1309\n",
      "    Batch 80/253, Loss: 1.2927\n",
      "    Batch 90/253, Loss: 1.1640\n",
      "    Batch 100/253, Loss: 1.2573\n",
      "    Batch 110/253, Loss: 1.1757\n",
      "    Batch 120/253, Loss: 0.8929\n",
      "    Batch 130/253, Loss: 1.2750\n",
      "    Batch 140/253, Loss: 1.3191\n",
      "    Batch 150/253, Loss: 1.3289\n",
      "    Batch 160/253, Loss: 0.8649\n",
      "    Batch 170/253, Loss: 1.0367\n",
      "    Batch 180/253, Loss: 1.0250\n",
      "    Batch 190/253, Loss: 0.9291\n",
      "    Batch 200/253, Loss: 0.9396\n",
      "    Batch 210/253, Loss: 1.2219\n",
      "    Batch 220/253, Loss: 1.0384\n",
      "    Batch 230/253, Loss: 0.9375\n",
      "    Batch 240/253, Loss: 1.2045\n",
      "    Batch 250/253, Loss: 0.8046\n",
      "\n",
      "  Train Loss: 1.0691\n",
      "  Val Loss:   0.9966\n",
      "  Val Acc:    0.6352\n",
      "  Time:       64.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6458\n",
      "    EMW: 0.4570\n",
      "    FSW: 0.7071\n",
      "    SSW: 0.3001\n",
      "    OWW: 0.8480\n",
      "  [Saved new best model]\n",
      "\n",
      "============================================================\n",
      "Training complete!\n",
      "Best validation loss: 0.9966\n",
      "Models saved to: Models\n"
     ]
    }
   ],
   "source": [
    "def compute_class_weights(data_dir, cluster_id, huc_id, class_names):\n",
    "    \"\"\"Compute class weights from training data using inverse frequency.\"\"\"\n",
    "    files = find_patch_files(data_dir, cluster_id, huc_id)\n",
    "    \n",
    "    print(\"Loading y_train files for class weight computation...\")\n",
    "    y_train_list = [np.load(f) for f in files['y_train']]\n",
    "    y_train = np.concatenate(y_train_list, axis=0)\n",
    "    print(f\"  Combined y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    # Count pixels per class\n",
    "    classes, counts = np.unique(y_train, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    \n",
    "    # Compute inverse frequency weights\n",
    "    frequencies = counts / total\n",
    "    weights = 1.0 / frequencies\n",
    "    weights = weights / weights.min()  # Normalize so smallest weight is 1.0\n",
    "    \n",
    "    print(\"\\nClass distribution and weights:\")\n",
    "    for c, count, w in zip(classes, counts, weights):\n",
    "        pct = count / total * 100\n",
    "        print(f\"  {class_names[c]}: {count:,} pixels ({pct:.2f}%) -> weight: {w:.2f}\")\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Device selection\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                          \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # === COMPUTE CLASS WEIGHTS ===\n",
    "    class_weights = compute_class_weights(\n",
    "        data_dir, cluster_id, huc_id, metadata[\"class_names\"]\n",
    "    )\n",
    "\n",
    "    # === CREATE MODEL ===\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = UNet(\n",
    "        in_channels=metadata[\"in_channels\"],\n",
    "        num_classes=metadata[\"num_classes\"],\n",
    "        base_filters=base_filters\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "    # === LOSS AND OPTIMIZER ===\n",
    "    class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # === TRAINING LOOP ===\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc, class_acc = validate(model, val_loader, criterion, device, metadata)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        # Log results\n",
    "        print(f\"\\n  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "        print(f\"  Val Acc:    {val_acc:.4f}\")\n",
    "        print(f\"  Time:       {epoch_time:.1f}s\")\n",
    "        print(\"  Per-class accuracy:\")\n",
    "        for name, acc in class_acc.items():\n",
    "            print(f\"    {name}: {acc:.4f}\")\n",
    "\n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'metadata': metadata,\n",
    "                'config': {\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'huc_id': huc_id,\n",
    "                    'base_filters': base_filters,\n",
    "                    'learning_rate': learning_rate,\n",
    "                }\n",
    "            }, output_dir / \"best_model.pth\")\n",
    "            print(\"  [Saved new best model]\")\n",
    "\n",
    "    # === SAVE FINAL MODEL AND HISTORY ===\n",
    "    torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'metadata': metadata,\n",
    "        'config': {\n",
    "            'cluster_id': cluster_id,\n",
    "            'huc_id': huc_id,\n",
    "            'base_filters': base_filters,\n",
    "            'learning_rate': learning_rate,\n",
    "        }\n",
    "    }, output_dir / \"final_model.pth\")\n",
    "\n",
    "    np.save(output_dir / \"training_history.npy\", history)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Models saved to: {output_dir}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# Run training (comment out to prevent execution)\n",
    "history = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ddb62c-fe9c-47ad-b686-6958e348857e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove"
    ]
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script Python_Code_Analysis/DL_Implement/NYS_06_train.ipynb --TagRemovePreprocessor.remove_cell_tags='{\"remove\"}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wetland-cnn",
   "language": "python",
   "name": "wetland-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
