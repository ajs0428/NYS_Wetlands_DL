{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5c3bf5-9201-4c7f-b5b6-7622df105d1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/Anthony/Data and Analysis Local/NYS_Wetlands_DL\n",
      "\n",
      "Data Configuration:\n",
      "  data_dir: Data/Patches_v2\n",
      "  cluster_id: 208\n",
      "  huc_id: All HUCs in cluster\n",
      "\n",
      "Training Configuration:\n",
      "  num_epochs: 25\n",
      "  batch_size: 10\n",
      "  learning_rate: 0.001\n",
      "  base_filters: 32\n",
      "  output_dir: Models\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "workdir = Path(\"/Users/Anthony/Data and Analysis Local/NYS_Wetlands_DL/\")\n",
    "os.chdir(workdir)\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Must match the output from NYS_03_create_patches_v2.ipynb\n",
    "data_dir = Path(\"Data/Patches_v2\")\n",
    "cluster_id = 208  # Cluster to load, or None for legacy files\n",
    "huc_id = None     # Specific HUC to load, or None to combine all HUCs in cluster\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 25\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "base_filters = 32  # U-Net base filter count\n",
    "\n",
    "# Output directory for models\n",
    "output_dir = Path(\"Models\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\nData Configuration:\")\n",
    "print(f\"  data_dir: {data_dir}\")\n",
    "print(f\"  cluster_id: {cluster_id}\")\n",
    "print(f\"  huc_id: {huc_id or 'All HUCs in cluster'}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  num_epochs: {num_epochs}\")\n",
    "print(f\"  batch_size: {batch_size}\")\n",
    "print(f\"  learning_rate: {learning_rate}\")\n",
    "print(f\"  base_filters: {base_filters}\")\n",
    "print(f\"  output_dir: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b38b106-673a-4eb4-8628-95db182d4e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Found 9 training file(s)\n",
      "Found 9 validation file(s)\n",
      "\n",
      "Dataset Summary:\n",
      "  Training batches: 284\n",
      "  Validation batches: 72\n",
      "  in_channels: 11\n",
      "  num_classes: 5\n",
      "  band_names: ['r', 'g', 'b', 'nir', 'ndvi', 'ndwi', 'dem', 'chm', 'slope_5m', 'TPI_5m', 'Geomorph_5m']\n",
      "  HUCs included: ['metadata', 'metadata', 'metadata', 'metadata', 'metadata', 'metadata', 'metadata', 'metadata', 'metadata']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Add script directory to Python path\n",
    "script_dir = Path(\"Python_Code_Analysis/DL_Implement/\")\n",
    "sys.path.insert(0, str(script_dir))\n",
    "\n",
    "# Import modules\n",
    "from NYS_04_dataset import get_dataloaders, find_patch_files, load_and_merge_metadata\n",
    "from NYS_05_unet_model import UNet\n",
    "\n",
    "# === LOAD DATA ===\n",
    "print(\"Loading data...\")\n",
    "train_loader, val_loader, metadata = get_dataloaders(\n",
    "    data_dir, \n",
    "    cluster_id=cluster_id, \n",
    "    huc_id=huc_id, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  in_channels: {metadata['in_channels']}\")\n",
    "print(f\"  num_classes: {metadata['num_classes']}\")\n",
    "print(f\"  band_names: {metadata['band_names']}\")\n",
    "if \"hucs_included\" in metadata:\n",
    "    print(f\"  HUCs included: {metadata['hucs_included']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c47603f5-d35e-44d1-89fc-07c44c8787df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove"
    ]
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch and return average loss.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Progress update every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"    Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, metadata):\n",
    "    \"\"\"Validate and return loss plus per-class accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    num_classes = metadata[\"num_classes\"]\n",
    "    class_names = metadata[\"class_names\"]\n",
    "\n",
    "    # Track correct predictions per class\n",
    "    correct_per_class = torch.zeros(num_classes)\n",
    "    total_per_class = torch.zeros(num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Per-class accuracy\n",
    "            for c in range(num_classes):\n",
    "                mask = (y == c)\n",
    "                total_per_class[c] += mask.sum().item()\n",
    "                correct_per_class[c] += ((preds == c) & mask).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "\n",
    "    # Calculate per-class accuracy\n",
    "    class_acc = {}\n",
    "    for c in range(num_classes):\n",
    "        if total_per_class[c] > 0:\n",
    "            class_acc[class_names[c]] = correct_per_class[c] / total_per_class[c]\n",
    "        else:\n",
    "            class_acc[class_names[c]] = 0.0\n",
    "\n",
    "    # Overall accuracy\n",
    "    overall_acc = correct_per_class.sum() / total_per_class.sum()\n",
    "\n",
    "    return avg_loss, overall_acc.item(), class_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8691be53-c8a1-4658-a540-61b84a1765ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading y_train files for class weight computation...\n",
      "  Combined y_train shape: (2835, 128, 128)\n",
      "\n",
      "Class distribution and weights:\n",
      "  Background: 38,018,953 pixels (81.85%) -> weight: 1.00\n",
      "  EMW: 1,389,629 pixels (2.99%) -> weight: 27.36\n",
      "  FSW: 5,005,829 pixels (10.78%) -> weight: 7.59\n",
      "  SSW: 1,249,340 pixels (2.69%) -> weight: 30.43\n",
      "  OWW: 784,889 pixels (1.69%) -> weight: 48.44\n",
      "\n",
      "Initializing model...\n",
      "Total parameters: 7,768,421\n",
      "\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "Epoch 1/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 1.2852\n",
      "    Batch 20/284, Loss: 1.4958\n",
      "    Batch 30/284, Loss: 0.9254\n",
      "    Batch 40/284, Loss: 1.3692\n",
      "    Batch 50/284, Loss: 1.1923\n",
      "    Batch 60/284, Loss: 1.0076\n",
      "    Batch 70/284, Loss: 1.3356\n",
      "    Batch 80/284, Loss: 1.3772\n",
      "    Batch 90/284, Loss: 1.3636\n",
      "    Batch 100/284, Loss: 1.4377\n",
      "    Batch 110/284, Loss: 1.2550\n",
      "    Batch 120/284, Loss: 0.9176\n",
      "    Batch 130/284, Loss: 1.1705\n",
      "    Batch 140/284, Loss: 1.0345\n",
      "    Batch 150/284, Loss: 1.1769\n",
      "    Batch 160/284, Loss: 1.3825\n",
      "    Batch 170/284, Loss: 1.6897\n",
      "    Batch 180/284, Loss: 1.3102\n",
      "    Batch 190/284, Loss: 0.8587\n",
      "    Batch 200/284, Loss: 1.2990\n",
      "    Batch 210/284, Loss: 1.0770\n",
      "    Batch 220/284, Loss: 1.0363\n",
      "    Batch 230/284, Loss: 1.0197\n",
      "    Batch 240/284, Loss: 1.1405\n",
      "    Batch 250/284, Loss: 1.1060\n",
      "    Batch 260/284, Loss: 1.6510\n",
      "    Batch 270/284, Loss: 1.1417\n",
      "    Batch 280/284, Loss: 0.9635\n",
      "\n",
      "  Train Loss: 1.2778\n",
      "  Val Loss:   1.1949\n",
      "  Val Acc:    0.4356\n",
      "  Time:       27.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.4063\n",
      "    EMW: 0.4472\n",
      "    FSW: 0.5794\n",
      "    SSW: 0.3894\n",
      "    OWW: 0.7762\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 2/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 1.0601\n",
      "    Batch 20/284, Loss: 1.2529\n",
      "    Batch 30/284, Loss: 0.9188\n",
      "    Batch 40/284, Loss: 1.0414\n",
      "    Batch 50/284, Loss: 1.0097\n",
      "    Batch 60/284, Loss: 1.3144\n",
      "    Batch 70/284, Loss: 0.6861\n",
      "    Batch 80/284, Loss: 1.2030\n",
      "    Batch 90/284, Loss: 1.1991\n",
      "    Batch 100/284, Loss: 1.3499\n",
      "    Batch 110/284, Loss: 1.5441\n",
      "    Batch 120/284, Loss: 1.1685\n",
      "    Batch 130/284, Loss: 0.9982\n",
      "    Batch 140/284, Loss: 1.0252\n",
      "    Batch 150/284, Loss: 1.3966\n",
      "    Batch 160/284, Loss: 1.2906\n",
      "    Batch 170/284, Loss: 0.8337\n",
      "    Batch 180/284, Loss: 1.2328\n",
      "    Batch 190/284, Loss: 1.1659\n",
      "    Batch 200/284, Loss: 0.8208\n",
      "    Batch 210/284, Loss: 0.8323\n",
      "    Batch 220/284, Loss: 0.8080\n",
      "    Batch 230/284, Loss: 1.0011\n",
      "    Batch 240/284, Loss: 0.7977\n",
      "    Batch 250/284, Loss: 1.0881\n",
      "    Batch 260/284, Loss: 0.9328\n",
      "    Batch 270/284, Loss: 1.2030\n",
      "    Batch 280/284, Loss: 0.5934\n",
      "\n",
      "  Train Loss: 1.1586\n",
      "  Val Loss:   1.1206\n",
      "  Val Acc:    0.5323\n",
      "  Time:       19.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.4995\n",
      "    EMW: 0.4914\n",
      "    FSW: 0.7986\n",
      "    SSW: 0.3177\n",
      "    OWW: 0.6864\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 3/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 1.1662\n",
      "    Batch 20/284, Loss: 1.3832\n",
      "    Batch 30/284, Loss: 0.6989\n",
      "    Batch 40/284, Loss: 1.0258\n",
      "    Batch 50/284, Loss: 1.0561\n",
      "    Batch 60/284, Loss: 0.7198\n",
      "    Batch 70/284, Loss: 1.3167\n",
      "    Batch 80/284, Loss: 1.4104\n",
      "    Batch 90/284, Loss: 1.2483\n",
      "    Batch 100/284, Loss: 0.8279\n",
      "    Batch 110/284, Loss: 1.0718\n",
      "    Batch 120/284, Loss: 0.7624\n",
      "    Batch 130/284, Loss: 1.4076\n",
      "    Batch 140/284, Loss: 1.0703\n",
      "    Batch 150/284, Loss: 0.8862\n",
      "    Batch 160/284, Loss: 0.8018\n",
      "    Batch 170/284, Loss: 1.3697\n",
      "    Batch 180/284, Loss: 0.7833\n",
      "    Batch 190/284, Loss: 1.0219\n",
      "    Batch 200/284, Loss: 0.7905\n",
      "    Batch 210/284, Loss: 0.9130\n",
      "    Batch 220/284, Loss: 0.8046\n",
      "    Batch 230/284, Loss: 0.7339\n",
      "    Batch 240/284, Loss: 0.8274\n",
      "    Batch 250/284, Loss: 1.2184\n",
      "    Batch 260/284, Loss: 1.3929\n",
      "    Batch 270/284, Loss: 0.7628\n",
      "    Batch 280/284, Loss: 0.9981\n",
      "\n",
      "  Train Loss: 1.1249\n",
      "  Val Loss:   1.0829\n",
      "  Val Acc:    0.5752\n",
      "  Time:       19.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.5550\n",
      "    EMW: 0.5753\n",
      "    FSW: 0.8416\n",
      "    SSW: 0.0710\n",
      "    OWW: 0.7377\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 4/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 1.3842\n",
      "    Batch 20/284, Loss: 1.1375\n",
      "    Batch 30/284, Loss: 0.9843\n",
      "    Batch 40/284, Loss: 1.1155\n",
      "    Batch 50/284, Loss: 1.1502\n",
      "    Batch 60/284, Loss: 0.9473\n",
      "    Batch 70/284, Loss: 0.7746\n",
      "    Batch 80/284, Loss: 1.0664\n",
      "    Batch 90/284, Loss: 1.0692\n",
      "    Batch 100/284, Loss: 1.2166\n",
      "    Batch 110/284, Loss: 0.8715\n",
      "    Batch 120/284, Loss: 1.3713\n",
      "    Batch 130/284, Loss: 1.7719\n",
      "    Batch 140/284, Loss: 0.9664\n",
      "    Batch 150/284, Loss: 0.6598\n",
      "    Batch 160/284, Loss: 1.2051\n",
      "    Batch 170/284, Loss: 1.0258\n",
      "    Batch 180/284, Loss: 1.0253\n",
      "    Batch 190/284, Loss: 1.1125\n",
      "    Batch 200/284, Loss: 0.8119\n",
      "    Batch 210/284, Loss: 1.0750\n",
      "    Batch 220/284, Loss: 1.4460\n",
      "    Batch 230/284, Loss: 1.3243\n",
      "    Batch 240/284, Loss: 0.7073\n",
      "    Batch 250/284, Loss: 1.3394\n",
      "    Batch 260/284, Loss: 1.3055\n",
      "    Batch 270/284, Loss: 1.0905\n",
      "    Batch 280/284, Loss: 1.0749\n",
      "\n",
      "  Train Loss: 1.0792\n",
      "  Val Loss:   1.0384\n",
      "  Val Acc:    0.6068\n",
      "  Time:       19.2s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6077\n",
      "    EMW: 0.3636\n",
      "    FSW: 0.6733\n",
      "    SSW: 0.4624\n",
      "    OWW: 0.7952\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 5/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.5611\n",
      "    Batch 20/284, Loss: 0.6799\n",
      "    Batch 30/284, Loss: 0.5498\n",
      "    Batch 40/284, Loss: 1.2489\n",
      "    Batch 50/284, Loss: 0.9751\n",
      "    Batch 60/284, Loss: 0.8143\n",
      "    Batch 70/284, Loss: 1.2422\n",
      "    Batch 80/284, Loss: 0.9672\n",
      "    Batch 90/284, Loss: 0.9246\n",
      "    Batch 100/284, Loss: 1.1399\n",
      "    Batch 110/284, Loss: 0.7548\n",
      "    Batch 120/284, Loss: 0.7564\n",
      "    Batch 130/284, Loss: 1.3123\n",
      "    Batch 140/284, Loss: 1.0217\n",
      "    Batch 150/284, Loss: 0.9707\n",
      "    Batch 160/284, Loss: 1.3763\n",
      "    Batch 170/284, Loss: 0.6989\n",
      "    Batch 180/284, Loss: 1.6112\n",
      "    Batch 190/284, Loss: 0.9277\n",
      "    Batch 200/284, Loss: 1.0784\n",
      "    Batch 210/284, Loss: 1.5511\n",
      "    Batch 220/284, Loss: 0.9376\n",
      "    Batch 230/284, Loss: 1.3176\n",
      "    Batch 240/284, Loss: 1.0741\n",
      "    Batch 250/284, Loss: 1.0573\n",
      "    Batch 260/284, Loss: 0.8936\n",
      "    Batch 270/284, Loss: 1.0569\n",
      "    Batch 280/284, Loss: 1.0593\n",
      "\n",
      "  Train Loss: 1.0724\n",
      "  Val Loss:   1.0758\n",
      "  Val Acc:    0.4821\n",
      "  Time:       19.4s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.4360\n",
      "    EMW: 0.4673\n",
      "    FSW: 0.7533\n",
      "    SSW: 0.4030\n",
      "    OWW: 0.8188\n",
      "\n",
      "Epoch 6/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 1.0508\n",
      "    Batch 20/284, Loss: 1.3342\n",
      "    Batch 30/284, Loss: 0.9102\n",
      "    Batch 40/284, Loss: 0.9427\n",
      "    Batch 50/284, Loss: 0.7067\n",
      "    Batch 60/284, Loss: 0.9380\n",
      "    Batch 70/284, Loss: 1.1421\n",
      "    Batch 80/284, Loss: 1.1243\n",
      "    Batch 90/284, Loss: 1.0580\n",
      "    Batch 100/284, Loss: 1.1355\n",
      "    Batch 110/284, Loss: 0.9396\n",
      "    Batch 120/284, Loss: 1.1456\n",
      "    Batch 130/284, Loss: 1.0280\n",
      "    Batch 140/284, Loss: 1.5467\n",
      "    Batch 150/284, Loss: 0.8281\n",
      "    Batch 160/284, Loss: 0.9618\n",
      "    Batch 170/284, Loss: 1.4493\n",
      "    Batch 180/284, Loss: 1.1874\n",
      "    Batch 190/284, Loss: 0.9895\n",
      "    Batch 200/284, Loss: 1.0046\n",
      "    Batch 210/284, Loss: 1.4586\n",
      "    Batch 220/284, Loss: 0.9230\n",
      "    Batch 230/284, Loss: 1.3947\n",
      "    Batch 240/284, Loss: 1.4539\n",
      "    Batch 250/284, Loss: 1.2962\n",
      "    Batch 260/284, Loss: 0.7668\n",
      "    Batch 270/284, Loss: 1.0816\n",
      "    Batch 280/284, Loss: 1.2667\n",
      "\n",
      "  Train Loss: 1.0719\n",
      "  Val Loss:   1.0244\n",
      "  Val Acc:    0.5944\n",
      "  Time:       19.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.5939\n",
      "    EMW: 0.6365\n",
      "    FSW: 0.7077\n",
      "    SSW: 0.1248\n",
      "    OWW: 0.7740\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 7/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 1.1894\n",
      "    Batch 20/284, Loss: 1.4686\n",
      "    Batch 30/284, Loss: 0.6781\n",
      "    Batch 40/284, Loss: 1.0486\n",
      "    Batch 50/284, Loss: 0.9928\n",
      "    Batch 60/284, Loss: 1.3174\n",
      "    Batch 70/284, Loss: 0.8660\n",
      "    Batch 80/284, Loss: 0.9212\n",
      "    Batch 90/284, Loss: 1.1703\n",
      "    Batch 100/284, Loss: 1.2818\n",
      "    Batch 110/284, Loss: 1.0708\n",
      "    Batch 120/284, Loss: 1.0994\n",
      "    Batch 130/284, Loss: 0.7188\n",
      "    Batch 140/284, Loss: 1.0446\n",
      "    Batch 150/284, Loss: 0.7233\n",
      "    Batch 160/284, Loss: 0.8501\n",
      "    Batch 170/284, Loss: 1.4748\n",
      "    Batch 180/284, Loss: 0.8286\n",
      "    Batch 190/284, Loss: 1.4042\n",
      "    Batch 200/284, Loss: 0.9665\n",
      "    Batch 210/284, Loss: 0.7359\n",
      "    Batch 220/284, Loss: 0.7957\n",
      "    Batch 230/284, Loss: 0.9862\n",
      "    Batch 240/284, Loss: 1.3088\n",
      "    Batch 250/284, Loss: 1.0366\n",
      "    Batch 260/284, Loss: 1.3168\n",
      "    Batch 270/284, Loss: 1.0747\n",
      "    Batch 280/284, Loss: 0.8898\n",
      "\n",
      "  Train Loss: 1.0603\n",
      "  Val Loss:   1.0233\n",
      "  Val Acc:    0.6284\n",
      "  Time:       19.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6254\n",
      "    EMW: 0.5887\n",
      "    FSW: 0.8013\n",
      "    SSW: 0.1319\n",
      "    OWW: 0.7236\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 8/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.8060\n",
      "    Batch 20/284, Loss: 1.0434\n",
      "    Batch 30/284, Loss: 0.9864\n",
      "    Batch 40/284, Loss: 0.8141\n",
      "    Batch 50/284, Loss: 1.2341\n",
      "    Batch 60/284, Loss: 1.3362\n",
      "    Batch 70/284, Loss: 0.8640\n",
      "    Batch 80/284, Loss: 0.7971\n",
      "    Batch 90/284, Loss: 0.7262\n",
      "    Batch 100/284, Loss: 0.6911\n",
      "    Batch 110/284, Loss: 1.3593\n",
      "    Batch 120/284, Loss: 1.6534\n",
      "    Batch 130/284, Loss: 0.8503\n",
      "    Batch 140/284, Loss: 0.8592\n",
      "    Batch 150/284, Loss: 0.9631\n",
      "    Batch 160/284, Loss: 0.7656\n",
      "    Batch 170/284, Loss: 1.2666\n",
      "    Batch 180/284, Loss: 1.3070\n",
      "    Batch 190/284, Loss: 0.7569\n",
      "    Batch 200/284, Loss: 0.8221\n",
      "    Batch 210/284, Loss: 0.9199\n",
      "    Batch 220/284, Loss: 1.3454\n",
      "    Batch 230/284, Loss: 1.4252\n",
      "    Batch 240/284, Loss: 0.9444\n",
      "    Batch 250/284, Loss: 0.9538\n",
      "    Batch 260/284, Loss: 1.0841\n",
      "    Batch 270/284, Loss: 0.8087\n",
      "    Batch 280/284, Loss: 0.8357\n",
      "\n",
      "  Train Loss: 1.0401\n",
      "  Val Loss:   1.0576\n",
      "  Val Acc:    0.5586\n",
      "  Time:       19.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.5456\n",
      "    EMW: 0.4586\n",
      "    FSW: 0.6986\n",
      "    SSW: 0.3191\n",
      "    OWW: 0.8238\n",
      "\n",
      "Epoch 9/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 1.1808\n",
      "    Batch 20/284, Loss: 1.0391\n",
      "    Batch 30/284, Loss: 1.1306\n",
      "    Batch 40/284, Loss: 1.2830\n",
      "    Batch 50/284, Loss: 0.7421\n",
      "    Batch 60/284, Loss: 0.9667\n",
      "    Batch 70/284, Loss: 0.8404\n",
      "    Batch 80/284, Loss: 0.8703\n",
      "    Batch 90/284, Loss: 0.8853\n",
      "    Batch 100/284, Loss: 1.1365\n",
      "    Batch 110/284, Loss: 1.0645\n",
      "    Batch 120/284, Loss: 1.6696\n",
      "    Batch 130/284, Loss: 1.2046\n",
      "    Batch 140/284, Loss: 0.9304\n",
      "    Batch 150/284, Loss: 0.8348\n",
      "    Batch 160/284, Loss: 1.2308\n",
      "    Batch 170/284, Loss: 1.0224\n",
      "    Batch 180/284, Loss: 1.2251\n",
      "    Batch 190/284, Loss: 0.9659\n",
      "    Batch 200/284, Loss: 1.3937\n",
      "    Batch 210/284, Loss: 1.0484\n",
      "    Batch 220/284, Loss: 0.7965\n",
      "    Batch 230/284, Loss: 0.8856\n",
      "    Batch 240/284, Loss: 1.1672\n",
      "    Batch 250/284, Loss: 1.0774\n",
      "    Batch 260/284, Loss: 0.6347\n",
      "    Batch 270/284, Loss: 1.2212\n",
      "    Batch 280/284, Loss: 1.1562\n",
      "\n",
      "  Train Loss: 1.0361\n",
      "  Val Loss:   1.0306\n",
      "  Val Acc:    0.5973\n",
      "  Time:       19.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6006\n",
      "    EMW: 0.6333\n",
      "    FSW: 0.6235\n",
      "    SSW: 0.3705\n",
      "    OWW: 0.6956\n",
      "\n",
      "Epoch 10/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 1.0324\n",
      "    Batch 20/284, Loss: 1.0679\n",
      "    Batch 30/284, Loss: 1.2849\n",
      "    Batch 40/284, Loss: 1.1216\n",
      "    Batch 50/284, Loss: 1.1320\n",
      "    Batch 60/284, Loss: 0.9185\n",
      "    Batch 70/284, Loss: 0.5960\n",
      "    Batch 80/284, Loss: 1.0670\n",
      "    Batch 90/284, Loss: 1.3362\n",
      "    Batch 100/284, Loss: 0.9954\n",
      "    Batch 110/284, Loss: 1.1375\n",
      "    Batch 120/284, Loss: 0.7228\n",
      "    Batch 130/284, Loss: 0.7602\n",
      "    Batch 140/284, Loss: 0.4424\n",
      "    Batch 150/284, Loss: 0.8278\n",
      "    Batch 160/284, Loss: 1.0014\n",
      "    Batch 170/284, Loss: 0.6709\n",
      "    Batch 180/284, Loss: 0.9960\n",
      "    Batch 190/284, Loss: 1.4276\n",
      "    Batch 200/284, Loss: 0.8748\n",
      "    Batch 210/284, Loss: 0.6634\n",
      "    Batch 220/284, Loss: 1.5355\n",
      "    Batch 230/284, Loss: 0.6290\n",
      "    Batch 240/284, Loss: 1.1356\n",
      "    Batch 250/284, Loss: 1.1299\n",
      "    Batch 260/284, Loss: 0.6440\n",
      "    Batch 270/284, Loss: 1.1590\n",
      "    Batch 280/284, Loss: 0.9585\n",
      "\n",
      "  Train Loss: 1.0438\n",
      "  Val Loss:   1.0508\n",
      "  Val Acc:    0.6325\n",
      "  Time:       19.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6525\n",
      "    EMW: 0.5478\n",
      "    FSW: 0.5984\n",
      "    SSW: 0.2976\n",
      "    OWW: 0.8120\n",
      "\n",
      "Epoch 11/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 1.1369\n",
      "    Batch 20/284, Loss: 1.0756\n",
      "    Batch 30/284, Loss: 0.9858\n",
      "    Batch 40/284, Loss: 1.7770\n",
      "    Batch 50/284, Loss: 1.3139\n",
      "    Batch 60/284, Loss: 1.0756\n",
      "    Batch 70/284, Loss: 0.9603\n",
      "    Batch 80/284, Loss: 1.0684\n",
      "    Batch 90/284, Loss: 0.6436\n",
      "    Batch 100/284, Loss: 0.8699\n",
      "    Batch 110/284, Loss: 1.0574\n",
      "    Batch 120/284, Loss: 1.0237\n",
      "    Batch 130/284, Loss: 1.3195\n",
      "    Batch 140/284, Loss: 0.6818\n",
      "    Batch 150/284, Loss: 0.7412\n",
      "    Batch 160/284, Loss: 1.1559\n",
      "    Batch 170/284, Loss: 1.5560\n",
      "    Batch 180/284, Loss: 1.3640\n",
      "    Batch 190/284, Loss: 1.1224\n",
      "    Batch 200/284, Loss: 0.9033\n",
      "    Batch 210/284, Loss: 1.2430\n",
      "    Batch 220/284, Loss: 2.2613\n",
      "    Batch 230/284, Loss: 0.6483\n",
      "    Batch 240/284, Loss: 0.8807\n",
      "    Batch 250/284, Loss: 1.2966\n",
      "    Batch 260/284, Loss: 0.9954\n",
      "    Batch 270/284, Loss: 1.2251\n",
      "    Batch 280/284, Loss: 0.4917\n",
      "\n",
      "  Train Loss: 1.0288\n",
      "  Val Loss:   1.0165\n",
      "  Val Acc:    0.6580\n",
      "  Time:       19.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6779\n",
      "    EMW: 0.5816\n",
      "    FSW: 0.6587\n",
      "    SSW: 0.2565\n",
      "    OWW: 0.7555\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 12/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 1.1979\n",
      "    Batch 20/284, Loss: 1.1068\n",
      "    Batch 30/284, Loss: 0.9829\n",
      "    Batch 40/284, Loss: 1.1721\n",
      "    Batch 50/284, Loss: 0.7847\n",
      "    Batch 60/284, Loss: 0.8927\n",
      "    Batch 70/284, Loss: 0.8846\n",
      "    Batch 80/284, Loss: 1.0295\n",
      "    Batch 90/284, Loss: 1.2417\n",
      "    Batch 100/284, Loss: 1.3141\n",
      "    Batch 110/284, Loss: 0.6251\n",
      "    Batch 120/284, Loss: 1.5002\n",
      "    Batch 130/284, Loss: 1.5073\n",
      "    Batch 140/284, Loss: 0.9494\n",
      "    Batch 150/284, Loss: 1.0105\n",
      "    Batch 160/284, Loss: 1.3003\n",
      "    Batch 170/284, Loss: 0.8046\n",
      "    Batch 180/284, Loss: 0.6665\n",
      "    Batch 190/284, Loss: 0.9989\n",
      "    Batch 200/284, Loss: 0.6459\n",
      "    Batch 210/284, Loss: 0.7652\n",
      "    Batch 220/284, Loss: 1.1340\n",
      "    Batch 230/284, Loss: 1.7410\n",
      "    Batch 240/284, Loss: 1.2779\n",
      "    Batch 250/284, Loss: 0.9793\n",
      "    Batch 260/284, Loss: 0.7018\n",
      "    Batch 270/284, Loss: 1.2165\n",
      "    Batch 280/284, Loss: 1.8364\n",
      "\n",
      "  Train Loss: 1.0181\n",
      "  Val Loss:   1.0421\n",
      "  Val Acc:    0.5605\n",
      "  Time:       19.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.5560\n",
      "    EMW: 0.6158\n",
      "    FSW: 0.5763\n",
      "    SSW: 0.4685\n",
      "    OWW: 0.7324\n",
      "\n",
      "Epoch 13/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.8996\n",
      "    Batch 20/284, Loss: 0.9374\n",
      "    Batch 30/284, Loss: 1.3392\n",
      "    Batch 40/284, Loss: 1.2070\n",
      "    Batch 50/284, Loss: 1.3598\n",
      "    Batch 60/284, Loss: 0.9987\n",
      "    Batch 70/284, Loss: 1.1767\n",
      "    Batch 80/284, Loss: 0.9989\n",
      "    Batch 90/284, Loss: 1.3035\n",
      "    Batch 100/284, Loss: 1.0181\n",
      "    Batch 110/284, Loss: 0.9859\n",
      "    Batch 120/284, Loss: 0.7489\n",
      "    Batch 130/284, Loss: 0.8191\n",
      "    Batch 140/284, Loss: 0.8928\n",
      "    Batch 150/284, Loss: 0.8110\n",
      "    Batch 160/284, Loss: 0.8332\n",
      "    Batch 170/284, Loss: 0.9624\n",
      "    Batch 180/284, Loss: 0.8576\n",
      "    Batch 190/284, Loss: 0.9634\n",
      "    Batch 200/284, Loss: 1.1698\n",
      "    Batch 210/284, Loss: 1.4930\n",
      "    Batch 220/284, Loss: 0.6491\n",
      "    Batch 230/284, Loss: 0.9118\n",
      "    Batch 240/284, Loss: 0.8760\n",
      "    Batch 250/284, Loss: 1.1202\n",
      "    Batch 260/284, Loss: 0.8337\n",
      "    Batch 270/284, Loss: 1.2051\n",
      "    Batch 280/284, Loss: 1.0324\n",
      "\n",
      "  Train Loss: 0.9941\n",
      "  Val Loss:   1.0137\n",
      "  Val Acc:    0.5877\n",
      "  Time:       19.2s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.5793\n",
      "    EMW: 0.6813\n",
      "    FSW: 0.6979\n",
      "    SSW: 0.2679\n",
      "    OWW: 0.7370\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 14/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 1.0538\n",
      "    Batch 20/284, Loss: 1.2164\n",
      "    Batch 30/284, Loss: 0.9849\n",
      "    Batch 40/284, Loss: 0.7630\n",
      "    Batch 50/284, Loss: 1.0049\n",
      "    Batch 60/284, Loss: 1.3039\n",
      "    Batch 70/284, Loss: 0.6170\n",
      "    Batch 80/284, Loss: 0.8364\n",
      "    Batch 90/284, Loss: 0.6614\n",
      "    Batch 100/284, Loss: 0.8953\n",
      "    Batch 110/284, Loss: 0.8060\n",
      "    Batch 120/284, Loss: 0.7851\n",
      "    Batch 130/284, Loss: 0.4225\n",
      "    Batch 140/284, Loss: 0.9837\n",
      "    Batch 150/284, Loss: 1.4935\n",
      "    Batch 160/284, Loss: 0.8882\n",
      "    Batch 170/284, Loss: 1.4324\n",
      "    Batch 180/284, Loss: 0.9990\n",
      "    Batch 190/284, Loss: 0.9339\n",
      "    Batch 200/284, Loss: 1.1305\n",
      "    Batch 210/284, Loss: 1.0467\n",
      "    Batch 220/284, Loss: 1.1443\n",
      "    Batch 230/284, Loss: 1.1518\n",
      "    Batch 240/284, Loss: 1.0863\n",
      "    Batch 250/284, Loss: 1.2139\n",
      "    Batch 260/284, Loss: 0.7045\n",
      "    Batch 270/284, Loss: 1.1071\n",
      "    Batch 280/284, Loss: 1.0734\n",
      "\n",
      "  Train Loss: 0.9991\n",
      "  Val Loss:   1.0076\n",
      "  Val Acc:    0.5636\n",
      "  Time:       19.2s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.5391\n",
      "    EMW: 0.6254\n",
      "    FSW: 0.7800\n",
      "    SSW: 0.2510\n",
      "    OWW: 0.7376\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 15/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.8993\n",
      "    Batch 20/284, Loss: 0.8171\n",
      "    Batch 30/284, Loss: 1.3189\n",
      "    Batch 40/284, Loss: 1.2592\n",
      "    Batch 50/284, Loss: 0.8810\n",
      "    Batch 60/284, Loss: 1.7370\n",
      "    Batch 70/284, Loss: 0.6692\n",
      "    Batch 80/284, Loss: 1.1169\n",
      "    Batch 90/284, Loss: 1.1581\n",
      "    Batch 100/284, Loss: 0.8783\n",
      "    Batch 110/284, Loss: 1.3815\n",
      "    Batch 120/284, Loss: 1.6773\n",
      "    Batch 130/284, Loss: 0.9062\n",
      "    Batch 140/284, Loss: 1.0048\n",
      "    Batch 150/284, Loss: 1.1770\n",
      "    Batch 160/284, Loss: 1.0366\n",
      "    Batch 170/284, Loss: 0.8166\n",
      "    Batch 180/284, Loss: 0.7986\n",
      "    Batch 190/284, Loss: 0.6782\n",
      "    Batch 200/284, Loss: 0.9974\n",
      "    Batch 210/284, Loss: 1.2902\n",
      "    Batch 220/284, Loss: 0.7598\n",
      "    Batch 230/284, Loss: 1.1751\n",
      "    Batch 240/284, Loss: 1.0291\n",
      "    Batch 250/284, Loss: 1.0562\n",
      "    Batch 260/284, Loss: 0.4462\n",
      "    Batch 270/284, Loss: 0.8156\n",
      "    Batch 280/284, Loss: 1.1270\n",
      "\n",
      "  Train Loss: 1.0055\n",
      "  Val Loss:   0.9869\n",
      "  Val Acc:    0.5512\n",
      "  Time:       19.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.5276\n",
      "    EMW: 0.6172\n",
      "    FSW: 0.6851\n",
      "    SSW: 0.4463\n",
      "    OWW: 0.7673\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 16/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.6254\n",
      "    Batch 20/284, Loss: 1.2490\n",
      "    Batch 30/284, Loss: 0.5948\n",
      "    Batch 40/284, Loss: 0.9948\n",
      "    Batch 50/284, Loss: 0.7412\n",
      "    Batch 60/284, Loss: 1.1252\n",
      "    Batch 70/284, Loss: 1.1808\n",
      "    Batch 80/284, Loss: 1.0804\n",
      "    Batch 90/284, Loss: 1.1723\n",
      "    Batch 100/284, Loss: 0.6838\n",
      "    Batch 110/284, Loss: 0.8392\n",
      "    Batch 120/284, Loss: 0.6640\n",
      "    Batch 130/284, Loss: 1.2548\n",
      "    Batch 140/284, Loss: 1.0721\n",
      "    Batch 150/284, Loss: 0.8114\n",
      "    Batch 160/284, Loss: 0.8354\n",
      "    Batch 170/284, Loss: 0.8509\n",
      "    Batch 180/284, Loss: 1.1651\n",
      "    Batch 190/284, Loss: 1.1886\n",
      "    Batch 200/284, Loss: 0.5261\n",
      "    Batch 210/284, Loss: 0.6985\n",
      "    Batch 220/284, Loss: 0.5040\n",
      "    Batch 230/284, Loss: 0.8744\n",
      "    Batch 240/284, Loss: 0.7346\n",
      "    Batch 250/284, Loss: 1.1096\n",
      "    Batch 260/284, Loss: 0.7597\n",
      "    Batch 270/284, Loss: 0.7840\n",
      "    Batch 280/284, Loss: 1.0903\n",
      "\n",
      "  Train Loss: 1.0041\n",
      "  Val Loss:   0.9884\n",
      "  Val Acc:    0.6230\n",
      "  Time:       20.6s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6330\n",
      "    EMW: 0.5033\n",
      "    FSW: 0.6021\n",
      "    SSW: 0.4888\n",
      "    OWW: 0.7801\n",
      "\n",
      "Epoch 17/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.8541\n",
      "    Batch 20/284, Loss: 1.1381\n",
      "    Batch 30/284, Loss: 0.7185\n",
      "    Batch 40/284, Loss: 0.7358\n",
      "    Batch 50/284, Loss: 0.8224\n",
      "    Batch 60/284, Loss: 0.9697\n",
      "    Batch 70/284, Loss: 1.3007\n",
      "    Batch 80/284, Loss: 0.9209\n",
      "    Batch 90/284, Loss: 0.9740\n",
      "    Batch 100/284, Loss: 0.5096\n",
      "    Batch 110/284, Loss: 1.0819\n",
      "    Batch 120/284, Loss: 0.9597\n",
      "    Batch 130/284, Loss: 1.5198\n",
      "    Batch 140/284, Loss: 0.8586\n",
      "    Batch 150/284, Loss: 0.6302\n",
      "    Batch 160/284, Loss: 0.6329\n",
      "    Batch 170/284, Loss: 0.6159\n",
      "    Batch 180/284, Loss: 0.6649\n",
      "    Batch 190/284, Loss: 1.0697\n",
      "    Batch 200/284, Loss: 0.9872\n",
      "    Batch 210/284, Loss: 1.1496\n",
      "    Batch 220/284, Loss: 0.6788\n",
      "    Batch 230/284, Loss: 0.9755\n",
      "    Batch 240/284, Loss: 0.5900\n",
      "    Batch 250/284, Loss: 0.5718\n",
      "    Batch 260/284, Loss: 0.8723\n",
      "    Batch 270/284, Loss: 0.9388\n",
      "    Batch 280/284, Loss: 0.9264\n",
      "\n",
      "  Train Loss: 0.9871\n",
      "  Val Loss:   0.9880\n",
      "  Val Acc:    0.6814\n",
      "  Time:       21.1s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7051\n",
      "    EMW: 0.5742\n",
      "    FSW: 0.6693\n",
      "    SSW: 0.3164\n",
      "    OWW: 0.6859\n",
      "\n",
      "Epoch 18/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.7360\n",
      "    Batch 20/284, Loss: 1.1654\n",
      "    Batch 30/284, Loss: 0.5414\n",
      "    Batch 40/284, Loss: 1.7552\n",
      "    Batch 50/284, Loss: 0.7330\n",
      "    Batch 60/284, Loss: 0.8969\n",
      "    Batch 70/284, Loss: 1.1177\n",
      "    Batch 80/284, Loss: 0.9085\n",
      "    Batch 90/284, Loss: 0.8447\n",
      "    Batch 100/284, Loss: 0.8696\n",
      "    Batch 110/284, Loss: 0.5463\n",
      "    Batch 120/284, Loss: 0.6708\n",
      "    Batch 130/284, Loss: 0.8693\n",
      "    Batch 140/284, Loss: 0.6015\n",
      "    Batch 150/284, Loss: 0.8207\n",
      "    Batch 160/284, Loss: 1.1630\n",
      "    Batch 170/284, Loss: 0.7947\n",
      "    Batch 180/284, Loss: 1.3934\n",
      "    Batch 190/284, Loss: 1.1160\n",
      "    Batch 200/284, Loss: 1.0039\n",
      "    Batch 210/284, Loss: 1.0724\n",
      "    Batch 220/284, Loss: 1.1920\n",
      "    Batch 230/284, Loss: 1.2154\n",
      "    Batch 240/284, Loss: 0.8115\n",
      "    Batch 250/284, Loss: 0.9594\n",
      "    Batch 260/284, Loss: 1.1986\n",
      "    Batch 270/284, Loss: 0.9974\n",
      "    Batch 280/284, Loss: 1.4800\n",
      "\n",
      "  Train Loss: 0.9823\n",
      "  Val Loss:   0.9930\n",
      "  Val Acc:    0.6640\n",
      "  Time:       21.0s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6766\n",
      "    EMW: 0.4842\n",
      "    FSW: 0.6976\n",
      "    SSW: 0.3785\n",
      "    OWW: 0.7757\n",
      "\n",
      "Epoch 19/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.8571\n",
      "    Batch 20/284, Loss: 0.9663\n",
      "    Batch 30/284, Loss: 0.6554\n",
      "    Batch 40/284, Loss: 0.7403\n",
      "    Batch 50/284, Loss: 0.6887\n",
      "    Batch 60/284, Loss: 0.9912\n",
      "    Batch 70/284, Loss: 0.9170\n",
      "    Batch 80/284, Loss: 1.2282\n",
      "    Batch 90/284, Loss: 1.5331\n",
      "    Batch 100/284, Loss: 1.9372\n",
      "    Batch 110/284, Loss: 0.8888\n",
      "    Batch 120/284, Loss: 1.2419\n",
      "    Batch 130/284, Loss: 0.9467\n",
      "    Batch 140/284, Loss: 0.9813\n",
      "    Batch 150/284, Loss: 0.7504\n",
      "    Batch 160/284, Loss: 1.2355\n",
      "    Batch 170/284, Loss: 0.6211\n",
      "    Batch 180/284, Loss: 1.0937\n",
      "    Batch 190/284, Loss: 1.0337\n",
      "    Batch 200/284, Loss: 0.9217\n",
      "    Batch 210/284, Loss: 1.1512\n",
      "    Batch 220/284, Loss: 0.9240\n",
      "    Batch 230/284, Loss: 1.3188\n",
      "    Batch 240/284, Loss: 1.2492\n",
      "    Batch 250/284, Loss: 0.9918\n",
      "    Batch 260/284, Loss: 0.8089\n",
      "    Batch 270/284, Loss: 1.0710\n",
      "    Batch 280/284, Loss: 0.7102\n",
      "\n",
      "  Train Loss: 0.9811\n",
      "  Val Loss:   0.9948\n",
      "  Val Acc:    0.6020\n",
      "  Time:       21.1s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.5885\n",
      "    EMW: 0.2806\n",
      "    FSW: 0.7693\n",
      "    SSW: 0.5068\n",
      "    OWW: 0.7665\n",
      "\n",
      "Epoch 20/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 1.0923\n",
      "    Batch 20/284, Loss: 1.2699\n",
      "    Batch 30/284, Loss: 0.5223\n",
      "    Batch 40/284, Loss: 1.1833\n",
      "    Batch 50/284, Loss: 2.0687\n",
      "    Batch 60/284, Loss: 0.9117\n",
      "    Batch 70/284, Loss: 0.5448\n",
      "    Batch 80/284, Loss: 1.1249\n",
      "    Batch 90/284, Loss: 0.8941\n",
      "    Batch 100/284, Loss: 0.5570\n",
      "    Batch 110/284, Loss: 0.6187\n",
      "    Batch 120/284, Loss: 0.6620\n",
      "    Batch 130/284, Loss: 0.7536\n",
      "    Batch 140/284, Loss: 1.0654\n",
      "    Batch 150/284, Loss: 1.0689\n",
      "    Batch 160/284, Loss: 0.8630\n",
      "    Batch 170/284, Loss: 1.4177\n",
      "    Batch 180/284, Loss: 1.1179\n",
      "    Batch 190/284, Loss: 1.0642\n",
      "    Batch 200/284, Loss: 0.7175\n",
      "    Batch 210/284, Loss: 0.8487\n",
      "    Batch 220/284, Loss: 0.8165\n",
      "    Batch 230/284, Loss: 1.0423\n",
      "    Batch 240/284, Loss: 1.0892\n",
      "    Batch 250/284, Loss: 0.8645\n",
      "    Batch 260/284, Loss: 0.8445\n",
      "    Batch 270/284, Loss: 1.1587\n",
      "    Batch 280/284, Loss: 0.9554\n",
      "\n",
      "  Train Loss: 0.9668\n",
      "  Val Loss:   0.9639\n",
      "  Val Acc:    0.5931\n",
      "  Time:       21.1s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.5839\n",
      "    EMW: 0.5003\n",
      "    FSW: 0.6701\n",
      "    SSW: 0.5101\n",
      "    OWW: 0.7786\n",
      "  [Saved new best model]\n",
      "\n",
      "Epoch 21/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.7613\n",
      "    Batch 20/284, Loss: 0.9017\n",
      "    Batch 30/284, Loss: 0.6663\n",
      "    Batch 40/284, Loss: 1.4443\n",
      "    Batch 50/284, Loss: 1.1699\n",
      "    Batch 60/284, Loss: 0.6836\n",
      "    Batch 70/284, Loss: 0.9460\n",
      "    Batch 80/284, Loss: 0.9013\n",
      "    Batch 90/284, Loss: 0.8063\n",
      "    Batch 100/284, Loss: 0.9572\n",
      "    Batch 110/284, Loss: 1.0101\n",
      "    Batch 120/284, Loss: 0.8696\n",
      "    Batch 130/284, Loss: 0.8413\n",
      "    Batch 140/284, Loss: 1.4664\n",
      "    Batch 150/284, Loss: 0.8513\n",
      "    Batch 160/284, Loss: 1.0094\n",
      "    Batch 170/284, Loss: 1.3584\n",
      "    Batch 180/284, Loss: 1.2894\n",
      "    Batch 190/284, Loss: 0.8725\n",
      "    Batch 200/284, Loss: 1.0940\n",
      "    Batch 210/284, Loss: 0.9422\n",
      "    Batch 220/284, Loss: 0.9025\n",
      "    Batch 230/284, Loss: 1.1937\n",
      "    Batch 240/284, Loss: 0.9363\n",
      "    Batch 250/284, Loss: 0.8565\n",
      "    Batch 260/284, Loss: 1.2901\n",
      "    Batch 270/284, Loss: 1.0365\n",
      "    Batch 280/284, Loss: 0.8411\n",
      "\n",
      "  Train Loss: 0.9770\n",
      "  Val Loss:   0.9845\n",
      "  Val Acc:    0.5926\n",
      "  Time:       20.0s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.5852\n",
      "    EMW: 0.6684\n",
      "    FSW: 0.7074\n",
      "    SSW: 0.2411\n",
      "    OWW: 0.7614\n",
      "\n",
      "Epoch 22/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.8989\n",
      "    Batch 20/284, Loss: 0.8727\n",
      "    Batch 30/284, Loss: 0.9508\n",
      "    Batch 40/284, Loss: 1.3352\n",
      "    Batch 50/284, Loss: 0.8881\n",
      "    Batch 60/284, Loss: 0.6245\n",
      "    Batch 70/284, Loss: 1.2963\n",
      "    Batch 80/284, Loss: 0.9402\n",
      "    Batch 90/284, Loss: 1.0762\n",
      "    Batch 100/284, Loss: 0.9785\n",
      "    Batch 110/284, Loss: 0.9152\n",
      "    Batch 120/284, Loss: 1.0329\n",
      "    Batch 130/284, Loss: 0.6074\n",
      "    Batch 140/284, Loss: 0.9110\n",
      "    Batch 150/284, Loss: 1.2889\n",
      "    Batch 160/284, Loss: 0.9452\n",
      "    Batch 170/284, Loss: 1.3488\n",
      "    Batch 180/284, Loss: 0.8435\n",
      "    Batch 190/284, Loss: 1.1908\n",
      "    Batch 200/284, Loss: 1.1703\n",
      "    Batch 210/284, Loss: 1.2906\n",
      "    Batch 220/284, Loss: 0.7921\n",
      "    Batch 230/284, Loss: 1.1404\n",
      "    Batch 240/284, Loss: 0.7302\n",
      "    Batch 250/284, Loss: 0.9114\n",
      "    Batch 260/284, Loss: 0.8218\n",
      "    Batch 270/284, Loss: 1.0517\n",
      "    Batch 280/284, Loss: 1.3099\n",
      "\n",
      "  Train Loss: 0.9939\n",
      "  Val Loss:   0.9941\n",
      "  Val Acc:    0.6095\n",
      "  Time:       19.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6153\n",
      "    EMW: 0.6239\n",
      "    FSW: 0.6091\n",
      "    SSW: 0.3916\n",
      "    OWW: 0.7753\n",
      "\n",
      "Epoch 23/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.7169\n",
      "    Batch 20/284, Loss: 1.4177\n",
      "    Batch 30/284, Loss: 0.8575\n",
      "    Batch 40/284, Loss: 1.0493\n",
      "    Batch 50/284, Loss: 0.9855\n",
      "    Batch 60/284, Loss: 1.0704\n",
      "    Batch 70/284, Loss: 0.5627\n",
      "    Batch 80/284, Loss: 1.0911\n",
      "    Batch 90/284, Loss: 0.6270\n",
      "    Batch 100/284, Loss: 0.5773\n",
      "    Batch 110/284, Loss: 0.8982\n",
      "    Batch 120/284, Loss: 0.8777\n",
      "    Batch 130/284, Loss: 0.8083\n",
      "    Batch 140/284, Loss: 1.0659\n",
      "    Batch 150/284, Loss: 0.8419\n",
      "    Batch 160/284, Loss: 0.8453\n",
      "    Batch 170/284, Loss: 1.3464\n",
      "    Batch 180/284, Loss: 0.6625\n",
      "    Batch 190/284, Loss: 0.8681\n",
      "    Batch 200/284, Loss: 0.5117\n",
      "    Batch 210/284, Loss: 0.7178\n",
      "    Batch 220/284, Loss: 1.0862\n",
      "    Batch 230/284, Loss: 1.1701\n",
      "    Batch 240/284, Loss: 0.7648\n",
      "    Batch 250/284, Loss: 1.3428\n",
      "    Batch 260/284, Loss: 1.1767\n",
      "    Batch 270/284, Loss: 0.6312\n",
      "    Batch 280/284, Loss: 0.6639\n",
      "\n",
      "  Train Loss: 0.9770\n",
      "  Val Loss:   0.9832\n",
      "  Val Acc:    0.6867\n",
      "  Time:       19.3s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7111\n",
      "    EMW: 0.4045\n",
      "    FSW: 0.6408\n",
      "    SSW: 0.5275\n",
      "    OWW: 0.7182\n",
      "\n",
      "Epoch 24/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.8218\n",
      "    Batch 20/284, Loss: 0.8988\n",
      "    Batch 30/284, Loss: 1.0549\n",
      "    Batch 40/284, Loss: 0.8643\n",
      "    Batch 50/284, Loss: 0.6926\n",
      "    Batch 60/284, Loss: 1.1019\n",
      "    Batch 70/284, Loss: 1.0966\n",
      "    Batch 80/284, Loss: 1.2871\n",
      "    Batch 90/284, Loss: 0.9777\n",
      "    Batch 100/284, Loss: 0.6874\n",
      "    Batch 110/284, Loss: 1.2464\n",
      "    Batch 120/284, Loss: 1.1598\n",
      "    Batch 130/284, Loss: 0.9383\n",
      "    Batch 140/284, Loss: 1.0770\n",
      "    Batch 150/284, Loss: 1.0092\n",
      "    Batch 160/284, Loss: 1.1057\n",
      "    Batch 170/284, Loss: 1.1882\n",
      "    Batch 180/284, Loss: 0.6251\n",
      "    Batch 190/284, Loss: 0.7068\n",
      "    Batch 200/284, Loss: 1.1485\n",
      "    Batch 210/284, Loss: 1.0737\n",
      "    Batch 220/284, Loss: 0.4733\n",
      "    Batch 230/284, Loss: 1.0512\n",
      "    Batch 240/284, Loss: 0.8509\n",
      "    Batch 250/284, Loss: 1.2629\n",
      "    Batch 260/284, Loss: 0.4445\n",
      "    Batch 270/284, Loss: 1.0734\n",
      "    Batch 280/284, Loss: 0.9208\n",
      "\n",
      "  Train Loss: 0.9664\n",
      "  Val Loss:   0.9993\n",
      "  Val Acc:    0.6443\n",
      "  Time:       19.4s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.6538\n",
      "    EMW: 0.5301\n",
      "    FSW: 0.6593\n",
      "    SSW: 0.4458\n",
      "    OWW: 0.7243\n",
      "\n",
      "Epoch 25/25\n",
      "----------------------------------------\n",
      "    Batch 10/284, Loss: 0.7893\n",
      "    Batch 20/284, Loss: 1.3907\n",
      "    Batch 30/284, Loss: 1.3219\n",
      "    Batch 40/284, Loss: 1.0675\n",
      "    Batch 50/284, Loss: 1.5221\n",
      "    Batch 60/284, Loss: 0.9556\n",
      "    Batch 70/284, Loss: 1.0800\n",
      "    Batch 80/284, Loss: 1.1399\n",
      "    Batch 90/284, Loss: 0.8905\n",
      "    Batch 100/284, Loss: 1.0767\n",
      "    Batch 110/284, Loss: 0.8971\n",
      "    Batch 120/284, Loss: 0.9320\n",
      "    Batch 130/284, Loss: 1.2990\n",
      "    Batch 140/284, Loss: 0.9041\n",
      "    Batch 150/284, Loss: 0.8888\n",
      "    Batch 160/284, Loss: 0.7810\n",
      "    Batch 170/284, Loss: 0.8826\n",
      "    Batch 180/284, Loss: 1.0556\n",
      "    Batch 190/284, Loss: 0.9362\n",
      "    Batch 200/284, Loss: 1.3199\n",
      "    Batch 210/284, Loss: 0.8079\n",
      "    Batch 220/284, Loss: 1.2702\n",
      "    Batch 230/284, Loss: 0.6998\n",
      "    Batch 240/284, Loss: 0.6689\n",
      "    Batch 250/284, Loss: 1.0171\n",
      "    Batch 260/284, Loss: 1.0113\n",
      "    Batch 270/284, Loss: 0.9547\n",
      "    Batch 280/284, Loss: 0.6947\n",
      "\n",
      "  Train Loss: 0.9693\n",
      "  Val Loss:   1.0185\n",
      "  Val Acc:    0.6951\n",
      "  Time:       19.2s\n",
      "  Per-class accuracy:\n",
      "    Background: 0.7373\n",
      "    EMW: 0.3876\n",
      "    FSW: 0.5414\n",
      "    SSW: 0.4929\n",
      "    OWW: 0.7908\n",
      "\n",
      "============================================================\n",
      "Training complete!\n",
      "Best validation loss: 0.9639\n",
      "Models saved to: Models\n"
     ]
    }
   ],
   "source": [
    "def compute_class_weights(data_dir, cluster_id, huc_id, class_names):\n",
    "    \"\"\"Compute class weights from training data using inverse frequency.\"\"\"\n",
    "    files = find_patch_files(data_dir, cluster_id, huc_id)\n",
    "    \n",
    "    print(\"Loading y_train files for class weight computation...\")\n",
    "    y_train_list = [np.load(f) for f in files['y_train']]\n",
    "    y_train = np.concatenate(y_train_list, axis=0)\n",
    "    print(f\"  Combined y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    # Count pixels per class\n",
    "    classes, counts = np.unique(y_train, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    \n",
    "    # Compute inverse frequency weights\n",
    "    frequencies = counts / total\n",
    "    weights = 1.0 / frequencies\n",
    "    weights = weights / weights.min()  # Normalize so smallest weight is 1.0\n",
    "    \n",
    "    print(\"\\nClass distribution and weights:\")\n",
    "    for c, count, w in zip(classes, counts, weights):\n",
    "        pct = count / total * 100\n",
    "        print(f\"  {class_names[c]}: {count:,} pixels ({pct:.2f}%) -> weight: {w:.2f}\")\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Device selection\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                          \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # === COMPUTE CLASS WEIGHTS ===\n",
    "    class_weights = compute_class_weights(\n",
    "        data_dir, cluster_id, huc_id, metadata[\"class_names\"]\n",
    "    )\n",
    "\n",
    "    # === CREATE MODEL ===\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = UNet(\n",
    "        in_channels=metadata[\"in_channels\"],\n",
    "        num_classes=metadata[\"num_classes\"],\n",
    "        base_filters=base_filters\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "    # === LOSS AND OPTIMIZER ===\n",
    "    class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # === TRAINING LOOP ===\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, val_acc, class_acc = validate(model, val_loader, criterion, device, metadata)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        # Log results\n",
    "        print(f\"\\n  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "        print(f\"  Val Acc:    {val_acc:.4f}\")\n",
    "        print(f\"  Time:       {epoch_time:.1f}s\")\n",
    "        print(\"  Per-class accuracy:\")\n",
    "        for name, acc in class_acc.items():\n",
    "            print(f\"    {name}: {acc:.4f}\")\n",
    "\n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'metadata': metadata,\n",
    "                'config': {\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'huc_id': huc_id,\n",
    "                    'base_filters': base_filters,\n",
    "                    'learning_rate': learning_rate,\n",
    "                }\n",
    "            }, output_dir / \"best_model.pth\")\n",
    "            print(\"  [Saved new best model]\")\n",
    "\n",
    "    # === SAVE FINAL MODEL AND HISTORY ===\n",
    "    torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'metadata': metadata,\n",
    "        'config': {\n",
    "            'cluster_id': cluster_id,\n",
    "            'huc_id': huc_id,\n",
    "            'base_filters': base_filters,\n",
    "            'learning_rate': learning_rate,\n",
    "        }\n",
    "    }, output_dir / \"final_model.pth\")\n",
    "\n",
    "    np.save(output_dir / \"training_history.npy\", history)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Models saved to: {output_dir}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# Run training (comment out to prevent execution)\n",
    "history = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4ddb62c-fe9c-47ad-b686-6958e348857e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Python_Code_Analysis/DL_Implement/NYS_06_train.ipynb to script\n",
      "[NbConvertApp] Writing 5586 bytes to Python_Code_Analysis/DL_Implement/NYS_06_train.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Python_Code_Analysis/DL_Implement/NYS_06_train.ipynb --TagRemovePreprocessor.remove_cell_tags='{\"remove\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480b069-aa8b-4293-9934-1e2224a66212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wetland-cnn",
   "language": "python",
   "name": "wetland-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
