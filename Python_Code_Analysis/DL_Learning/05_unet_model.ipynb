{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af908d99-ee5e-49da-89d4-ea4d983539d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ibstorage/anthony/NYS_Wetlands_GHG\n",
      "Current working directory is now: /ibstorage/anthony/NYS_Wetlands_GHG\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "workdir = Path(\"/ibstorage/anthony/NYS_Wetlands_GHG/\")\n",
    "print(workdir)\n",
    "os.chdir(workdir)\n",
    "current_working_dir = Path.cwd()\n",
    "print(f\"Current working directory is now: {current_working_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f41d3f3-6971-4503-b222-b9d0a963b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e02b45f-7d99-4e31-8ab4-b50782275046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Two consecutive conv layers with BatchNorm and ReLU.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"ConvBlock followed by MaxPool for downsampling.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x)\n",
    "        pooled = self.pool(conv_out)\n",
    "        return conv_out, pooled  # Return both for skip connection\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Upsample, concatenate skip connection, then ConvBlock.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(\n",
    "            in_channels, out_channels, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.conv = ConvBlock(out_channels * 2, out_channels)  # *2 for concatenation\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip], dim=1)  # Concatenate along channel dimension\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"Lightweight U-Net for semantic segmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=8, num_classes=5, base_filters=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Number of input bands (8: R,G,B,NIR,NDWI,NDVI,DEM, CHM)\n",
    "            num_classes: Number of output classes (5: background + 4 wetland types)\n",
    "            base_filters: Number of filters in first layer (doubles each level)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        f = base_filters  # 32\n",
    "        \n",
    "        # Encoder path\n",
    "        self.enc1 = EncoderBlock(in_channels, f)      # 7 -> 32\n",
    "        self.enc2 = EncoderBlock(f, f * 2)            # 32 -> 64\n",
    "        self.enc3 = EncoderBlock(f * 2, f * 4)        # 64 -> 128\n",
    "        self.enc4 = EncoderBlock(f * 4, f * 8)        # 128 -> 256\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(f * 8, f * 16)    # 256 -> 512\n",
    "        \n",
    "        # Decoder path\n",
    "        self.dec4 = DecoderBlock(f * 16, f * 8)       # 512 -> 256\n",
    "        self.dec3 = DecoderBlock(f * 8, f * 4)        # 256 -> 128\n",
    "        self.dec2 = DecoderBlock(f * 4, f * 2)        # 128 -> 64\n",
    "        self.dec1 = DecoderBlock(f * 2, f)            # 64 -> 32\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.final = nn.Conv2d(f, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        skip1, x = self.enc1(x)   # skip1: 32 channels\n",
    "        skip2, x = self.enc2(x)   # skip2: 64 channels\n",
    "        skip3, x = self.enc3(x)   # skip3: 128 channels\n",
    "        skip4, x = self.enc4(x)   # skip4: 256 channels\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)    # 512 channels\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.dec4(x, skip4)   # 256 channels\n",
    "        x = self.dec3(x, skip3)   # 128 channels\n",
    "        x = self.dec2(x, skip2)   # 64 channels\n",
    "        x = self.dec1(x, skip1)   # 32 channels\n",
    "        \n",
    "        # Output\n",
    "        return self.final(x)      # num_classes channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf05a0e2-f875-4280-a437-094906090d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 7,767,557\n",
      "Trainable parameters: 7,767,557\n",
      "\n",
      "Input shape: torch.Size([4, 8, 128, 128])\n",
      "Output shape: torch.Size([4, 5, 128, 128])\n",
      "\n",
      "Model architecture verified successfully!\n"
     ]
    }
   ],
   "source": [
    "# === TEST THE MODEL ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model\n",
    "    model = UNet(in_channels=8, num_classes=5, base_filters=32)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    dummy_input = torch.randn(4, 8, 128, 128)  # Batch of 4, 7 channels, 128x128\n",
    "    print(f\"\\nInput shape: {dummy_input.shape}\")\n",
    "    \n",
    "    output = model(dummy_input)\n",
    "    print(f\"Output shape: {output.shape}\")  # Should be (4, 5, 128, 128)\n",
    "    \n",
    "    # Verify output is correct shape for our task\n",
    "    assert output.shape == (4, 5, 128, 128), \"Output shape mismatch!\"\n",
    "    print(\"\\nModel architecture verified successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ccfbbba-395e-4f6e-abe7-7701a2a2ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "  Class 0: 15,398,159 pixels (82.15%)\n",
      "  Class 1: 691,767 pixels (3.69%)\n",
      "  Class 2: 1,195,304 pixels (6.38%)\n",
      "  Class 3: 1,180,713 pixels (6.30%)\n",
      "  Class 4: 277,353 pixels (1.48%)\n",
      "\n",
      "Class weights (inverse frequency):\n",
      "  Background: 1.00\n",
      "  EMW: 22.26\n",
      "  FSW: 12.88\n",
      "  SSW: 13.04\n",
      "  OWW: 55.52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train = np.load(\"Data/Patches_v2/y_train.npy\")\n",
    "\n",
    "# Count pixels per class\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "total = counts.sum()\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "for c, count in zip(classes, counts):\n",
    "    print(f\"  Class {c}: {count:,} pixels ({count/total*100:.2f}%)\")\n",
    "\n",
    "# Compute inverse frequency weights\n",
    "# Higher weight for rarer classes\n",
    "frequencies = counts / total\n",
    "weights = 1.0 / frequencies\n",
    "weights = weights / weights.min()  # Normalize so smallest weight is 1.0\n",
    "\n",
    "print(\"\\nClass weights (inverse frequency):\")\n",
    "class_names = ['Background', 'EMW', 'FSW', 'SSW', 'OWW']\n",
    "for c, w in zip(classes, weights):\n",
    "    print(f\"  {class_names[c]}: {w:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e686c65-6a7f-45ed-81a5-ea1a178b6ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wetland-cnn",
   "language": "python",
   "name": "wetland-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
