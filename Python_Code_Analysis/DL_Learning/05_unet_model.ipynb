{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af908d99-ee5e-49da-89d4-ea4d983539d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/Anthony/Data and Analysis Local/NYS_Wetlands_GHG\n",
      "\n",
      "Metadata loaded:\n",
      "  in_channels: 11\n",
      "  num_classes: 5\n",
      "  patch_size: 256\n",
      "  band_names: ['r', 'g', 'b', 'nir', 'ndvi', 'ndwi', 'dem', 'chm', 'slope_5m', 'TPI_5m', 'Geomorph_5m']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "workdir = Path(\"/Users/Anthony/Data and Analysis Local/NYS_Wetlands_GHG/\")\n",
    "os.chdir(workdir)\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "\n",
    "def load_metadata(data_dir=\"Data/Patches_v2\"):\n",
    "    \"\"\"Load metadata from patches directory.\"\"\"\n",
    "    metadata_path = Path(data_dir) / \"metadata.json\"\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path) as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Metadata not found at {metadata_path}. Run 03_create_patches_v2.ipynb first.\")\n",
    "\n",
    "# Load metadata\n",
    "metadata = load_metadata()\n",
    "print(f\"\\nMetadata loaded:\")\n",
    "print(f\"  in_channels: {metadata['in_channels']}\")\n",
    "print(f\"  num_classes: {metadata['num_classes']}\")\n",
    "print(f\"  patch_size: {metadata['patch_size']}\")\n",
    "print(f\"  band_names: {metadata['band_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f41d3f3-6971-4503-b222-b9d0a963b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e02b45f-7d99-4e31-8ab4-b50782275046",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Two consecutive conv layers with BatchNorm and ReLU.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"ConvBlock followed by MaxPool for downsampling.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x)\n",
    "        pooled = self.pool(conv_out)\n",
    "        return conv_out, pooled  # Return both for skip connection\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Upsample, concatenate skip connection, then ConvBlock.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(\n",
    "            in_channels, out_channels, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.conv = ConvBlock(out_channels * 2, out_channels)  # *2 for concatenation\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip], dim=1)  # Concatenate along channel dimension\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"Lightweight U-Net for semantic segmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, num_classes, base_filters=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Number of input bands (from metadata)\n",
    "            num_classes: Number of output classes (from metadata)\n",
    "            base_filters: Number of filters in first layer (doubles each level)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        f = base_filters  # 32\n",
    "        \n",
    "        # Encoder path\n",
    "        self.enc1 = EncoderBlock(in_channels, f)\n",
    "        self.enc2 = EncoderBlock(f, f * 2)\n",
    "        self.enc3 = EncoderBlock(f * 2, f * 4)\n",
    "        self.enc4 = EncoderBlock(f * 4, f * 8)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(f * 8, f * 16)\n",
    "        \n",
    "        # Decoder path\n",
    "        self.dec4 = DecoderBlock(f * 16, f * 8)\n",
    "        self.dec3 = DecoderBlock(f * 8, f * 4)\n",
    "        self.dec2 = DecoderBlock(f * 4, f * 2)\n",
    "        self.dec1 = DecoderBlock(f * 2, f)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.final = nn.Conv2d(f, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        skip1, x = self.enc1(x)\n",
    "        skip2, x = self.enc2(x)\n",
    "        skip3, x = self.enc3(x)\n",
    "        skip4, x = self.enc4(x)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.dec4(x, skip4)\n",
    "        x = self.dec3(x, skip3)\n",
    "        x = self.dec2(x, skip2)\n",
    "        x = self.dec1(x, skip1)\n",
    "        \n",
    "        # Output\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf05a0e2-f875-4280-a437-094906090d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 7,768,421\n",
      "Trainable parameters: 7,768,421\n",
      "\n",
      "Input shape: torch.Size([4, 11, 256, 256])\n",
      "Output shape: torch.Size([4, 5, 256, 256])\n",
      "\n",
      "Model architecture verified successfully!\n"
     ]
    }
   ],
   "source": [
    "# === TEST THE MODEL ===\n",
    "in_channels = metadata[\"in_channels\"]\n",
    "num_classes = metadata[\"num_classes\"]\n",
    "patch_size = metadata[\"patch_size\"]\n",
    "\n",
    "# Create model using metadata\n",
    "model = UNet(in_channels=in_channels, num_classes=num_classes, base_filters=32)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(4, in_channels, patch_size, patch_size)\n",
    "print(f\"\\nInput shape: {dummy_input.shape}\")\n",
    "\n",
    "output = model(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Verify output is correct shape\n",
    "expected_shape = (4, num_classes, patch_size, patch_size)\n",
    "assert output.shape == expected_shape, f\"Output shape mismatch! Expected {expected_shape}, got {output.shape}\"\n",
    "print(\"\\nModel architecture verified successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ccfbbba-395e-4f6e-abe7-7701a2a2ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "  Class 0: 16,512,968 pixels (83.43%)\n",
      "  Class 1: 705,145 pixels (3.56%)\n",
      "  Class 2: 1,171,525 pixels (5.92%)\n",
      "  Class 3: 1,144,319 pixels (5.78%)\n",
      "  Class 4: 257,915 pixels (1.30%)\n",
      "\n",
      "Class weights (inverse frequency):\n",
      "  Background: 1.00\n",
      "  EMW: 23.42\n",
      "  FSW: 14.10\n",
      "  SSW: 14.43\n",
      "  OWW: 64.02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train = np.load(\"Data/Patches_v2/y_train.npy\")\n",
    "\n",
    "# Count pixels per class\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "total = counts.sum()\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "for c, count in zip(classes, counts):\n",
    "    print(f\"  Class {c}: {count:,} pixels ({count/total*100:.2f}%)\")\n",
    "\n",
    "# Compute inverse frequency weights\n",
    "frequencies = counts / total\n",
    "weights = 1.0 / frequencies\n",
    "weights = weights / weights.min()\n",
    "\n",
    "print(\"\\nClass weights (inverse frequency):\")\n",
    "class_names = metadata[\"class_names\"]\n",
    "for c, w in zip(classes, weights):\n",
    "    print(f\"  {class_names[c]}: {w:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e686c65-6a7f-45ed-81a5-ea1a178b6ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wetland-cnn",
   "language": "python",
   "name": "wetland-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
